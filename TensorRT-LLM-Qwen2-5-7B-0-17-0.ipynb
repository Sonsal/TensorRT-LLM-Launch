{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac316c9b-6bd4-44bc-92ca-8455107156f2",
   "metadata": {},
   "source": [
    "# Getting started with TensorRT-LLM\n",
    "\n",
    "In this notebook we are optimizing Bloom 560M for inference using TensorRT-LLM. <br>\n",
    "We compare inference speed results from the **baseline** model (Huggingface), the **optimized** model, <br>\n",
    "and the optimized model after **INT8-quantization**.\n",
    "\n",
    "***Source:***\n",
    "https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/bloom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45885a26-ad28-473d-83d8-5c45bfc176d2",
   "metadata": {},
   "source": [
    "### Installing the NVIDIA Container Toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44df7ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c01a848-2619-4d7e-ad0a-d48529837c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg' exists. \u001b[?2004hOverwrite? (y/N) \n",
      "gpg: signal 2 caught ... exiting\n",
      "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
      "Get:2 https://nvidia.github.io/libnvidia-container/stable/deb/amd64  InRelease [1477 B]\n",
      "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease                         \n",
      "Hit:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "Get:7 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1244 kB]\n",
      "Get:8 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2844 kB]\n",
      "Hit:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Fetched 4218 kB in 1s (3260 kB/s)\n",
      "Reading package lists... Done\n"
     ]
    }
   ],
   "source": [
    "!curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey |  gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\\n",
    "  && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\\n",
    "    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\\n",
    "     tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\n",
    "\n",
    "!apt-get update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9e37e9-ef50-4e81-bab2-9c3ad4a94586",
   "metadata": {},
   "source": [
    "### Installing TensorRT-LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b6333ce-4c4f-4fe9-b08f-c6db2242e00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'TensorRT-LLM'...\n",
      "remote: Enumerating objects: 58011, done.\u001b[K\n",
      "remote: Counting objects: 100% (196/196), done.\u001b[K\n",
      "remote: Compressing objects: 100% (110/110), done.\u001b[K\n",
      "remote: Total 58011 (delta 134), reused 86 (delta 86), pack-reused 57815 (from 3)\u001b[K\n",
      "Receiving objects: 100% (58011/58011), 1.10 GiB | 18.02 MiB/s, done.\n",
      "Resolving deltas: 100% (44085/44085), done.\n",
      "Note: switching to '258c7540c03517def55d9a5aadfa9288af474e1b'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by switching back to a branch.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -c with the switch command. Example:\n",
      "\n",
      "  git switch -c <new-branch-name>\n",
      "\n",
      "Or undo this operation with:\n",
      "\n",
      "  git switch -\n",
      "\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\n",
      "\n",
      "Updating files: 100% (4515/4515), done.\n",
      "Filtering content: 100% (21/21), 841.64 MiB | 1.77 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone --branch \"v0.17.0\" https://github.com/NVIDIA/TensorRT-LLM/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f3f644f-c715-4262-8adc-4f67b9695566",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]\n",
      "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]      \n",
      "Get:3 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]        \n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]      \n",
      "Get:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [34.3 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB]\n",
      "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1581 B]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB]    \n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4272 kB]\n",
      "Get:13 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4118 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1543 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [55.7 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3155 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [82.7 kB]\n",
      "Get:19 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1244 kB]\n",
      "Get:20 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [47.7 kB]\n",
      "Get:21 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2844 kB]\n",
      "Get:22 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1605 kB]\n",
      "Fetched 39.4 MB in 3s (14.5 MB/s)                           \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  autoconf automake autotools-dev file gfortran gfortran-11 ibverbs-providers\n",
      "  javascript-common libcaf-openmpi-3 libcoarrays-dev libcoarrays-openmpi-dev\n",
      "  libevent-2.1-7 libevent-core-2.1-7 libevent-dev libevent-extra-2.1-7\n",
      "  libevent-openssl-2.1-7 libevent-pthreads-2.1-7 libfabric1 libgfortran-11-dev\n",
      "  libgfortran5 libhwloc-dev libhwloc-plugins libhwloc15 libibverbs-dev\n",
      "  libibverbs1 libjs-jquery libjs-jquery-ui libjs-sphinxdoc libjs-underscore\n",
      "  libltdl-dev libltdl7 libmagic-mgc libmagic1 libnl-3-200 libnl-3-dev\n",
      "  libnl-route-3-200 libnl-route-3-dev libnuma-dev libnuma1 libopenmpi3\n",
      "  libpmix-dev libpmix2 libpsm-infinipath1 libpsm2-2 libpython3-dev\n",
      "  libpython3-stdlib libpython3.10 libpython3.10-dev libpython3.10-minimal\n",
      "  libpython3.10-stdlib librdmacm1 libsigsegv2 libtool libucx0 libxnvctrl0 m4\n",
      "  ocl-icd-libopencl1 openmpi-common python3 python3-dev python3-minimal\n",
      "  python3-pkg-resources python3-setuptools python3-wheel python3.10-dev\n",
      "  python3.10-minimal python3.10-venv\n",
      "Suggested packages:\n",
      "  autoconf-archive gnu-standards autoconf-doc gettext gfortran-multilib\n",
      "  gfortran-doc gfortran-11-multilib gfortran-11-doc libhwloc-contrib-plugins\n",
      "  libjs-jquery-ui-docs libtool-doc openmpi-doc gcj-jdk m4-doc opencl-icd\n",
      "  python3-doc python3-tk python3-venv python-setuptools-doc python3.10-doc\n",
      "  binfmt-support\n",
      "The following NEW packages will be installed:\n",
      "  autoconf automake autotools-dev file gfortran gfortran-11 ibverbs-providers\n",
      "  javascript-common libcaf-openmpi-3 libcoarrays-dev libcoarrays-openmpi-dev\n",
      "  libevent-2.1-7 libevent-core-2.1-7 libevent-dev libevent-extra-2.1-7\n",
      "  libevent-openssl-2.1-7 libevent-pthreads-2.1-7 libfabric1 libgfortran-11-dev\n",
      "  libgfortran5 libhwloc-dev libhwloc-plugins libhwloc15 libibverbs-dev\n",
      "  libibverbs1 libjs-jquery libjs-jquery-ui libjs-sphinxdoc libjs-underscore\n",
      "  libltdl-dev libltdl7 libmagic-mgc libmagic1 libnl-3-200 libnl-3-dev\n",
      "  libnl-route-3-200 libnl-route-3-dev libnuma-dev libnuma1 libopenmpi-dev\n",
      "  libopenmpi3 libpmix-dev libpmix2 libpsm-infinipath1 libpsm2-2 libpython3-dev\n",
      "  librdmacm1 libsigsegv2 libtool libucx0 libxnvctrl0 m4 ocl-icd-libopencl1\n",
      "  openmpi-bin openmpi-common python3-dev python3-pip python3-setuptools\n",
      "  python3-wheel\n",
      "The following packages will be upgraded:\n",
      "  libpython3-stdlib libpython3.10 libpython3.10-dev libpython3.10-minimal\n",
      "  libpython3.10-stdlib python3 python3-minimal python3-pkg-resources\n",
      "  python3.10 python3.10-dev python3.10-minimal python3.10-venv\n",
      "12 upgraded, 59 newly installed, 0 to remove and 131 not upgraded.\n",
      "Need to get 40.5 MB of archives.\n",
      "After this operation, 117 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3-minimal amd64 3.10.6-1~22.04.1 [24.3 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3 amd64 3.10.6-1~22.04.1 [22.8 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3.10-dev amd64 3.10.12-1~22.04.9 [508 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpython3.10-dev amd64 3.10.12-1~22.04.9 [4763 kB]\n",
      "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libxnvctrl0 570.133.20-0ubuntu1 [11.8 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpython3.10 amd64 3.10.12-1~22.04.9 [1949 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3.10-venv amd64 3.10.12-1~22.04.9 [5722 B]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3.10 amd64 3.10.12-1~22.04.9 [508 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3.10-minimal amd64 3.10.12-1~22.04.9 [2263 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpython3.10-stdlib amd64 3.10.12-1~22.04.9 [1850 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpython3.10-minimal amd64 3.10.12-1~22.04.9 [815 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpython3-stdlib amd64 3.10.6-1~22.04.1 [6812 B]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3-pkg-resources all 59.6.0-1.2ubuntu0.22.04.2 [133 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libmagic-mgc amd64 1:5.41-3ubuntu0.1 [257 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libmagic1 amd64 1:5.41-3ubuntu0.1 [87.2 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 file amd64 1:5.41-3ubuntu0.1 [21.5 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy/main amd64 libnuma1 amd64 2.0.14-3ubuntu2 [22.5 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsigsegv2 amd64 2.13-1ubuntu3 [14.6 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu jammy/main amd64 m4 amd64 1.4.18-5ubuntu2 [199 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu jammy/main amd64 autoconf all 2.71-2 [338 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu jammy/main amd64 autotools-dev all 20220109.1 [44.9 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu jammy/main amd64 automake all 1:1.16.5-1.3 [558 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgfortran5 amd64 12.3.0-1ubuntu1~22.04 [879 kB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgfortran-11-dev amd64 11.4.0-1ubuntu1~22.04 [842 kB]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gfortran-11 amd64 11.4.0-1ubuntu1~22.04 [11.2 MB]\n",
      "Get:26 http://archive.ubuntu.com/ubuntu jammy/main amd64 gfortran amd64 4:11.2.0-1ubuntu1 [1182 B]\n",
      "Get:27 http://archive.ubuntu.com/ubuntu jammy/main amd64 libnl-3-200 amd64 3.5.0-0.1 [59.1 kB]\n",
      "Get:28 http://archive.ubuntu.com/ubuntu jammy/main amd64 libnl-route-3-200 amd64 3.5.0-0.1 [180 kB]\n",
      "Get:29 http://archive.ubuntu.com/ubuntu jammy/main amd64 libibverbs1 amd64 39.0-1 [69.3 kB]\n",
      "Get:30 http://archive.ubuntu.com/ubuntu jammy/main amd64 ibverbs-providers amd64 39.0-1 [341 kB]\n",
      "Get:31 http://archive.ubuntu.com/ubuntu jammy/main amd64 javascript-common all 11+nmu1 [5936 B]\n",
      "Get:32 http://archive.ubuntu.com/ubuntu jammy/main amd64 libevent-core-2.1-7 amd64 2.1.12-stable-1build3 [93.9 kB]\n",
      "Get:33 http://archive.ubuntu.com/ubuntu jammy/main amd64 libevent-pthreads-2.1-7 amd64 2.1.12-stable-1build3 [7642 B]\n",
      "Get:34 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libpsm-infinipath1 amd64 3.3+20.604758e7-6.1 [170 kB]\n",
      "Get:35 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libpsm2-2 amd64 11.2.185-1 [182 kB]\n",
      "Get:36 http://archive.ubuntu.com/ubuntu jammy/main amd64 librdmacm1 amd64 39.0-1 [71.2 kB]\n",
      "Get:37 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libfabric1 amd64 1.11.0-3 [558 kB]\n",
      "Get:38 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libhwloc15 amd64 2.7.0-2ubuntu1 [159 kB]\n",
      "Get:39 http://archive.ubuntu.com/ubuntu jammy/universe amd64 ocl-icd-libopencl1 amd64 2.2.14-3 [39.1 kB]\n",
      "Get:40 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libhwloc-plugins amd64 2.7.0-2ubuntu1 [15.6 kB]\n",
      "Get:41 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libpmix2 amd64 4.1.2-2ubuntu1 [604 kB]\n",
      "Get:42 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libucx0 amd64 1.12.1~rc2-1 [891 kB]\n",
      "Get:43 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopenmpi3 amd64 4.1.2-2ubuntu1 [2594 kB]\n",
      "Get:44 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcaf-openmpi-3 amd64 2.9.2-3 [36.5 kB]\n",
      "Get:45 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcoarrays-dev amd64 2.9.2-3 [40.5 kB]\n",
      "Get:46 http://archive.ubuntu.com/ubuntu jammy/universe amd64 openmpi-common all 4.1.2-2ubuntu1 [162 kB]\n",
      "Get:47 http://archive.ubuntu.com/ubuntu jammy/universe amd64 openmpi-bin amd64 4.1.2-2ubuntu1 [116 kB]\n",
      "Get:48 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcoarrays-openmpi-dev amd64 2.9.2-3 [452 kB]\n",
      "Get:49 http://archive.ubuntu.com/ubuntu jammy/main amd64 libevent-2.1-7 amd64 2.1.12-stable-1build3 [148 kB]\n",
      "Get:50 http://archive.ubuntu.com/ubuntu jammy/main amd64 libevent-extra-2.1-7 amd64 2.1.12-stable-1build3 [65.4 kB]\n",
      "Get:51 http://archive.ubuntu.com/ubuntu jammy/main amd64 libevent-openssl-2.1-7 amd64 2.1.12-stable-1build3 [15.8 kB]\n",
      "Get:52 http://archive.ubuntu.com/ubuntu jammy/main amd64 libevent-dev amd64 2.1.12-stable-1build3 [278 kB]\n",
      "Get:53 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjs-jquery all 3.6.0+dfsg+~3.5.13-1 [321 kB]\n",
      "Get:54 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libjs-jquery-ui all 1.13.1+dfsg-1 [253 kB]\n",
      "Get:55 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjs-underscore all 1.13.2~dfsg-2 [118 kB]\n",
      "Get:56 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjs-sphinxdoc all 4.3.2-1 [139 kB]\n",
      "Get:57 http://archive.ubuntu.com/ubuntu jammy/main amd64 libltdl7 amd64 2.4.6-15build2 [39.6 kB]\n",
      "Get:58 http://archive.ubuntu.com/ubuntu jammy/main amd64 libltdl-dev amd64 2.4.6-15build2 [169 kB]\n",
      "Get:59 http://archive.ubuntu.com/ubuntu jammy/main amd64 libnl-3-dev amd64 3.5.0-0.1 [101 kB]\n",
      "Get:60 http://archive.ubuntu.com/ubuntu jammy/main amd64 libnl-route-3-dev amd64 3.5.0-0.1 [202 kB]\n",
      "Get:61 http://archive.ubuntu.com/ubuntu jammy/main amd64 libnuma-dev amd64 2.0.14-3ubuntu2 [35.9 kB]\n",
      "Get:62 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libhwloc-dev amd64 2.7.0-2ubuntu1 [256 kB]\n",
      "Get:63 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libpmix-dev amd64 4.1.2-2ubuntu1 [805 kB]\n",
      "Get:64 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpython3-dev amd64 3.10.6-1~22.04.1 [7064 B]\n",
      "Get:65 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtool all 2.4.6-15build2 [164 kB]\n",
      "Get:66 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3-dev amd64 3.10.6-1~22.04.1 [26.0 kB]\n",
      "Get:67 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3-setuptools all 59.6.0-1.2ubuntu0.22.04.2 [340 kB]\n",
      "Get:68 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-wheel all 0.37.1-2ubuntu0.22.04.1 [32.0 kB]\n",
      "Get:69 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-pip all 22.0.2+dfsg-1ubuntu0.5 [1306 kB]\n",
      "Get:70 http://archive.ubuntu.com/ubuntu jammy/main amd64 libibverbs-dev amd64 39.0-1 [628 kB]\n",
      "Get:71 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopenmpi-dev amd64 4.1.2-2ubuntu1 [867 kB]\n",
      "Fetched 40.5 MB in 3s (13.0 MB/s)        \n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "(Reading database ... 20729 files and directories currently installed.)\n",
      "Preparing to unpack .../python3-minimal_3.10.6-1~22.04.1_amd64.deb ...\n",
      "Unpacking python3-minimal (3.10.6-1~22.04.1) over (3.10.6-1~22.04) ...\n",
      "Setting up python3-minimal (3.10.6-1~22.04.1) ...\n",
      "(Reading database ... 20729 files and directories currently installed.)\n",
      "Preparing to unpack .../00-python3_3.10.6-1~22.04.1_amd64.deb ...\n",
      "Unpacking python3 (3.10.6-1~22.04.1) over (3.10.6-1~22.04) ...\n",
      "Preparing to unpack .../01-python3.10-dev_3.10.12-1~22.04.9_amd64.deb ...\n",
      "Unpacking python3.10-dev (3.10.12-1~22.04.9) over (3.10.12-1~22.04.2) ...\n",
      "Preparing to unpack .../02-libpython3.10-dev_3.10.12-1~22.04.9_amd64.deb ...\n",
      "Unpacking libpython3.10-dev:amd64 (3.10.12-1~22.04.9) over (3.10.12-1~22.04.2) ...\n",
      "Preparing to unpack .../03-libpython3.10_3.10.12-1~22.04.9_amd64.deb ...\n",
      "Unpacking libpython3.10:amd64 (3.10.12-1~22.04.9) over (3.10.12-1~22.04.2) ...\n",
      "Preparing to unpack .../04-python3.10-venv_3.10.12-1~22.04.9_amd64.deb ...\n",
      "Unpacking python3.10-venv (3.10.12-1~22.04.9) over (3.10.12-1~22.04.2) ...\n",
      "Preparing to unpack .../05-python3.10_3.10.12-1~22.04.9_amd64.deb ...\n",
      "Unpacking python3.10 (3.10.12-1~22.04.9) over (3.10.12-1~22.04.2) ...\n",
      "Preparing to unpack .../06-python3.10-minimal_3.10.12-1~22.04.9_amd64.deb ...\n",
      "Unpacking python3.10-minimal (3.10.12-1~22.04.9) over (3.10.12-1~22.04.2) ...\n",
      "Preparing to unpack .../07-libpython3.10-stdlib_3.10.12-1~22.04.9_amd64.deb ...\n",
      "Unpacking libpython3.10-stdlib:amd64 (3.10.12-1~22.04.9) over (3.10.12-1~22.04.2) ...\n",
      "Preparing to unpack .../08-libpython3.10-minimal_3.10.12-1~22.04.9_amd64.deb ...\n",
      "Unpacking libpython3.10-minimal:amd64 (3.10.12-1~22.04.9) over (3.10.12-1~22.04.2) ...\n",
      "Preparing to unpack .../09-libpython3-stdlib_3.10.6-1~22.04.1_amd64.deb ...\n",
      "Unpacking libpython3-stdlib:amd64 (3.10.6-1~22.04.1) over (3.10.6-1~22.04) ...\n",
      "Preparing to unpack .../10-python3-pkg-resources_59.6.0-1.2ubuntu0.22.04.2_all.deb ...\n",
      "Unpacking python3-pkg-resources (59.6.0-1.2ubuntu0.22.04.2) over (59.6.0-1.2ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libmagic-mgc.\n",
      "Preparing to unpack .../11-libmagic-mgc_1%3a5.41-3ubuntu0.1_amd64.deb ...\n",
      "Unpacking libmagic-mgc (1:5.41-3ubuntu0.1) ...\n",
      "Selecting previously unselected package libmagic1:amd64.\n",
      "Preparing to unpack .../12-libmagic1_1%3a5.41-3ubuntu0.1_amd64.deb ...\n",
      "Unpacking libmagic1:amd64 (1:5.41-3ubuntu0.1) ...\n",
      "Selecting previously unselected package file.\n",
      "Preparing to unpack .../13-file_1%3a5.41-3ubuntu0.1_amd64.deb ...\n",
      "Unpacking file (1:5.41-3ubuntu0.1) ...\n",
      "Selecting previously unselected package libnuma1:amd64.\n",
      "Preparing to unpack .../14-libnuma1_2.0.14-3ubuntu2_amd64.deb ...\n",
      "Unpacking libnuma1:amd64 (2.0.14-3ubuntu2) ...\n",
      "Selecting previously unselected package libsigsegv2:amd64.\n",
      "Preparing to unpack .../15-libsigsegv2_2.13-1ubuntu3_amd64.deb ...\n",
      "Unpacking libsigsegv2:amd64 (2.13-1ubuntu3) ...\n",
      "Selecting previously unselected package m4.\n",
      "Preparing to unpack .../16-m4_1.4.18-5ubuntu2_amd64.deb ...\n",
      "Unpacking m4 (1.4.18-5ubuntu2) ...\n",
      "Selecting previously unselected package autoconf.\n",
      "Preparing to unpack .../17-autoconf_2.71-2_all.deb ...\n",
      "Unpacking autoconf (2.71-2) ...\n",
      "Selecting previously unselected package autotools-dev.\n",
      "Preparing to unpack .../18-autotools-dev_20220109.1_all.deb ...\n",
      "Unpacking autotools-dev (20220109.1) ...\n",
      "Selecting previously unselected package automake.\n",
      "Preparing to unpack .../19-automake_1%3a1.16.5-1.3_all.deb ...\n",
      "Unpacking automake (1:1.16.5-1.3) ...\n",
      "Selecting previously unselected package libgfortran5:amd64.\n",
      "Preparing to unpack .../20-libgfortran5_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
      "Unpacking libgfortran5:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
      "Selecting previously unselected package libgfortran-11-dev:amd64.\n",
      "Preparing to unpack .../21-libgfortran-11-dev_11.4.0-1ubuntu1~22.04_amd64.deb ...\n",
      "Unpacking libgfortran-11-dev:amd64 (11.4.0-1ubuntu1~22.04) ...\n",
      "Selecting previously unselected package gfortran-11.\n",
      "Preparing to unpack .../22-gfortran-11_11.4.0-1ubuntu1~22.04_amd64.deb ...\n",
      "Unpacking gfortran-11 (11.4.0-1ubuntu1~22.04) ...\n",
      "Selecting previously unselected package gfortran.\n",
      "Preparing to unpack .../23-gfortran_4%3a11.2.0-1ubuntu1_amd64.deb ...\n",
      "Unpacking gfortran (4:11.2.0-1ubuntu1) ...\n",
      "Selecting previously unselected package libnl-3-200:amd64.\n",
      "Preparing to unpack .../24-libnl-3-200_3.5.0-0.1_amd64.deb ...\n",
      "Unpacking libnl-3-200:amd64 (3.5.0-0.1) ...\n",
      "Selecting previously unselected package libnl-route-3-200:amd64.\n",
      "Preparing to unpack .../25-libnl-route-3-200_3.5.0-0.1_amd64.deb ...\n",
      "Unpacking libnl-route-3-200:amd64 (3.5.0-0.1) ...\n",
      "Selecting previously unselected package libibverbs1:amd64.\n",
      "Preparing to unpack .../26-libibverbs1_39.0-1_amd64.deb ...\n",
      "Unpacking libibverbs1:amd64 (39.0-1) ...\n",
      "Selecting previously unselected package ibverbs-providers:amd64.\n",
      "Preparing to unpack .../27-ibverbs-providers_39.0-1_amd64.deb ...\n",
      "Unpacking ibverbs-providers:amd64 (39.0-1) ...\n",
      "Selecting previously unselected package javascript-common.\n",
      "Preparing to unpack .../28-javascript-common_11+nmu1_all.deb ...\n",
      "Unpacking javascript-common (11+nmu1) ...\n",
      "Selecting previously unselected package libevent-core-2.1-7:amd64.\n",
      "Preparing to unpack .../29-libevent-core-2.1-7_2.1.12-stable-1build3_amd64.deb ...\n",
      "Unpacking libevent-core-2.1-7:amd64 (2.1.12-stable-1build3) ...\n",
      "Selecting previously unselected package libevent-pthreads-2.1-7:amd64.\n",
      "Preparing to unpack .../30-libevent-pthreads-2.1-7_2.1.12-stable-1build3_amd64.deb ...\n",
      "Unpacking libevent-pthreads-2.1-7:amd64 (2.1.12-stable-1build3) ...\n",
      "Selecting previously unselected package libpsm-infinipath1.\n",
      "Preparing to unpack .../31-libpsm-infinipath1_3.3+20.604758e7-6.1_amd64.deb ...\n",
      "Unpacking libpsm-infinipath1 (3.3+20.604758e7-6.1) ...\n",
      "Selecting previously unselected package libpsm2-2.\n",
      "Preparing to unpack .../32-libpsm2-2_11.2.185-1_amd64.deb ...\n",
      "Unpacking libpsm2-2 (11.2.185-1) ...\n",
      "Selecting previously unselected package librdmacm1:amd64.\n",
      "Preparing to unpack .../33-librdmacm1_39.0-1_amd64.deb ...\n",
      "Unpacking librdmacm1:amd64 (39.0-1) ...\n",
      "Selecting previously unselected package libfabric1:amd64.\n",
      "Preparing to unpack .../34-libfabric1_1.11.0-3_amd64.deb ...\n",
      "Unpacking libfabric1:amd64 (1.11.0-3) ...\n",
      "Selecting previously unselected package libhwloc15:amd64.\n",
      "Preparing to unpack .../35-libhwloc15_2.7.0-2ubuntu1_amd64.deb ...\n",
      "Unpacking libhwloc15:amd64 (2.7.0-2ubuntu1) ...\n",
      "Selecting previously unselected package libxnvctrl0:amd64.\n",
      "Preparing to unpack .../36-libxnvctrl0_570.133.20-0ubuntu1_amd64.deb ...\n",
      "Unpacking libxnvctrl0:amd64 (570.133.20-0ubuntu1) ...\n",
      "Selecting previously unselected package ocl-icd-libopencl1:amd64.\n",
      "Preparing to unpack .../37-ocl-icd-libopencl1_2.2.14-3_amd64.deb ...\n",
      "Unpacking ocl-icd-libopencl1:amd64 (2.2.14-3) ...\n",
      "Selecting previously unselected package libhwloc-plugins:amd64.\n",
      "Preparing to unpack .../38-libhwloc-plugins_2.7.0-2ubuntu1_amd64.deb ...\n",
      "Unpacking libhwloc-plugins:amd64 (2.7.0-2ubuntu1) ...\n",
      "Selecting previously unselected package libpmix2:amd64.\n",
      "Preparing to unpack .../39-libpmix2_4.1.2-2ubuntu1_amd64.deb ...\n",
      "Unpacking libpmix2:amd64 (4.1.2-2ubuntu1) ...\n",
      "Selecting previously unselected package libucx0:amd64.\n",
      "Preparing to unpack .../40-libucx0_1.12.1~rc2-1_amd64.deb ...\n",
      "Unpacking libucx0:amd64 (1.12.1~rc2-1) ...\n",
      "Selecting previously unselected package libopenmpi3:amd64.\n",
      "Preparing to unpack .../41-libopenmpi3_4.1.2-2ubuntu1_amd64.deb ...\n",
      "Unpacking libopenmpi3:amd64 (4.1.2-2ubuntu1) ...\n",
      "Selecting previously unselected package libcaf-openmpi-3:amd64.\n",
      "Preparing to unpack .../42-libcaf-openmpi-3_2.9.2-3_amd64.deb ...\n",
      "Unpacking libcaf-openmpi-3:amd64 (2.9.2-3) ...\n",
      "Selecting previously unselected package libcoarrays-dev:amd64.\n",
      "Preparing to unpack .../43-libcoarrays-dev_2.9.2-3_amd64.deb ...\n",
      "Unpacking libcoarrays-dev:amd64 (2.9.2-3) ...\n",
      "Selecting previously unselected package openmpi-common.\n",
      "Preparing to unpack .../44-openmpi-common_4.1.2-2ubuntu1_all.deb ...\n",
      "Unpacking openmpi-common (4.1.2-2ubuntu1) ...\n",
      "Selecting previously unselected package openmpi-bin.\n",
      "Preparing to unpack .../45-openmpi-bin_4.1.2-2ubuntu1_amd64.deb ...\n",
      "Unpacking openmpi-bin (4.1.2-2ubuntu1) ...\n",
      "Selecting previously unselected package libcoarrays-openmpi-dev:amd64.\n",
      "Preparing to unpack .../46-libcoarrays-openmpi-dev_2.9.2-3_amd64.deb ...\n",
      "Unpacking libcoarrays-openmpi-dev:amd64 (2.9.2-3) ...\n",
      "Selecting previously unselected package libevent-2.1-7:amd64.\n",
      "Preparing to unpack .../47-libevent-2.1-7_2.1.12-stable-1build3_amd64.deb ...\n",
      "Unpacking libevent-2.1-7:amd64 (2.1.12-stable-1build3) ...\n",
      "Selecting previously unselected package libevent-extra-2.1-7:amd64.\n",
      "Preparing to unpack .../48-libevent-extra-2.1-7_2.1.12-stable-1build3_amd64.deb ...\n",
      "Unpacking libevent-extra-2.1-7:amd64 (2.1.12-stable-1build3) ...\n",
      "Selecting previously unselected package libevent-openssl-2.1-7:amd64.\n",
      "Preparing to unpack .../49-libevent-openssl-2.1-7_2.1.12-stable-1build3_amd64.deb ...\n",
      "Unpacking libevent-openssl-2.1-7:amd64 (2.1.12-stable-1build3) ...\n",
      "Selecting previously unselected package libevent-dev.\n",
      "Preparing to unpack .../50-libevent-dev_2.1.12-stable-1build3_amd64.deb ...\n",
      "Unpacking libevent-dev (2.1.12-stable-1build3) ...\n",
      "Selecting previously unselected package libjs-jquery.\n",
      "Preparing to unpack .../51-libjs-jquery_3.6.0+dfsg+~3.5.13-1_all.deb ...\n",
      "Unpacking libjs-jquery (3.6.0+dfsg+~3.5.13-1) ...\n",
      "Selecting previously unselected package libjs-jquery-ui.\n",
      "Preparing to unpack .../52-libjs-jquery-ui_1.13.1+dfsg-1_all.deb ...\n",
      "Unpacking libjs-jquery-ui (1.13.1+dfsg-1) ...\n",
      "Selecting previously unselected package libjs-underscore.\n",
      "Preparing to unpack .../53-libjs-underscore_1.13.2~dfsg-2_all.deb ...\n",
      "Unpacking libjs-underscore (1.13.2~dfsg-2) ...\n",
      "Selecting previously unselected package libjs-sphinxdoc.\n",
      "Preparing to unpack .../54-libjs-sphinxdoc_4.3.2-1_all.deb ...\n",
      "Unpacking libjs-sphinxdoc (4.3.2-1) ...\n",
      "Selecting previously unselected package libltdl7:amd64.\n",
      "Preparing to unpack .../55-libltdl7_2.4.6-15build2_amd64.deb ...\n",
      "Unpacking libltdl7:amd64 (2.4.6-15build2) ...\n",
      "Selecting previously unselected package libltdl-dev:amd64.\n",
      "Preparing to unpack .../56-libltdl-dev_2.4.6-15build2_amd64.deb ...\n",
      "Unpacking libltdl-dev:amd64 (2.4.6-15build2) ...\n",
      "Selecting previously unselected package libnl-3-dev:amd64.\n",
      "Preparing to unpack .../57-libnl-3-dev_3.5.0-0.1_amd64.deb ...\n",
      "Unpacking libnl-3-dev:amd64 (3.5.0-0.1) ...\n",
      "Selecting previously unselected package libnl-route-3-dev:amd64.\n",
      "Preparing to unpack .../58-libnl-route-3-dev_3.5.0-0.1_amd64.deb ...\n",
      "Unpacking libnl-route-3-dev:amd64 (3.5.0-0.1) ...\n",
      "Selecting previously unselected package libnuma-dev:amd64.\n",
      "Preparing to unpack .../59-libnuma-dev_2.0.14-3ubuntu2_amd64.deb ...\n",
      "Unpacking libnuma-dev:amd64 (2.0.14-3ubuntu2) ...\n",
      "Selecting previously unselected package libhwloc-dev:amd64.\n",
      "Preparing to unpack .../60-libhwloc-dev_2.7.0-2ubuntu1_amd64.deb ...\n",
      "Unpacking libhwloc-dev:amd64 (2.7.0-2ubuntu1) ...\n",
      "Selecting previously unselected package libpmix-dev:amd64.\n",
      "Preparing to unpack .../61-libpmix-dev_4.1.2-2ubuntu1_amd64.deb ...\n",
      "Unpacking libpmix-dev:amd64 (4.1.2-2ubuntu1) ...\n",
      "Selecting previously unselected package libpython3-dev:amd64.\n",
      "Preparing to unpack .../62-libpython3-dev_3.10.6-1~22.04.1_amd64.deb ...\n",
      "Unpacking libpython3-dev:amd64 (3.10.6-1~22.04.1) ...\n",
      "Selecting previously unselected package libtool.\n",
      "Preparing to unpack .../63-libtool_2.4.6-15build2_all.deb ...\n",
      "Unpacking libtool (2.4.6-15build2) ...\n",
      "Selecting previously unselected package python3-dev.\n",
      "Preparing to unpack .../64-python3-dev_3.10.6-1~22.04.1_amd64.deb ...\n",
      "Unpacking python3-dev (3.10.6-1~22.04.1) ...\n",
      "Selecting previously unselected package python3-setuptools.\n",
      "Preparing to unpack .../65-python3-setuptools_59.6.0-1.2ubuntu0.22.04.2_all.deb ...\n",
      "Unpacking python3-setuptools (59.6.0-1.2ubuntu0.22.04.2) ...\n",
      "Selecting previously unselected package python3-wheel.\n",
      "Preparing to unpack .../66-python3-wheel_0.37.1-2ubuntu0.22.04.1_all.deb ...\n",
      "Unpacking python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package python3-pip.\n",
      "Preparing to unpack .../67-python3-pip_22.0.2+dfsg-1ubuntu0.5_all.deb ...\n",
      "Unpacking python3-pip (22.0.2+dfsg-1ubuntu0.5) ...\n",
      "Selecting previously unselected package libibverbs-dev:amd64.\n",
      "Preparing to unpack .../68-libibverbs-dev_39.0-1_amd64.deb ...\n",
      "Unpacking libibverbs-dev:amd64 (39.0-1) ...\n",
      "Selecting previously unselected package libopenmpi-dev:amd64.\n",
      "Preparing to unpack .../69-libopenmpi-dev_4.1.2-2ubuntu1_amd64.deb ...\n",
      "Unpacking libopenmpi-dev:amd64 (4.1.2-2ubuntu1) ...\n",
      "Setting up javascript-common (11+nmu1) ...\n",
      "Setting up libmagic-mgc (1:5.41-3ubuntu0.1) ...\n",
      "Setting up libmagic1:amd64 (1:5.41-3ubuntu0.1) ...\n",
      "Setting up file (1:5.41-3ubuntu0.1) ...\n",
      "Setting up libxnvctrl0:amd64 (570.133.20-0ubuntu1) ...\n",
      "Setting up autotools-dev (20220109.1) ...\n",
      "Setting up libsigsegv2:amd64 (2.13-1ubuntu3) ...\n",
      "Setting up libhwloc15:amd64 (2.7.0-2ubuntu1) ...\n",
      "Setting up libevent-core-2.1-7:amd64 (2.1.12-stable-1build3) ...\n",
      "Setting up libevent-2.1-7:amd64 (2.1.12-stable-1build3) ...\n",
      "Setting up libpython3.10-minimal:amd64 (3.10.12-1~22.04.9) ...\n",
      "Setting up libltdl7:amd64 (2.4.6-15build2) ...\n",
      "Setting up libgfortran5:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
      "Setting up libnuma1:amd64 (2.0.14-3ubuntu2) ...\n",
      "Setting up ocl-icd-libopencl1:amd64 (2.2.14-3) ...\n",
      "Setting up libnl-3-200:amd64 (3.5.0-0.1) ...\n",
      "Setting up libpsm2-2 (11.2.185-1) ...\n",
      "Setting up openmpi-common (4.1.2-2ubuntu1) ...\n",
      "Setting up libpsm-infinipath1 (3.3+20.604758e7-6.1) ...\n",
      "update-alternatives: using /usr/lib/libpsm1/libpsm_infinipath.so.1.16 to provide /usr/lib/x86_64-linux-gnu/libpsm_infinipath.so.1 (libpsm_infinipath.so.1) in auto mode\n",
      "Setting up libjs-jquery (3.6.0+dfsg+~3.5.13-1) ...\n",
      "Setting up libjs-underscore (1.13.2~dfsg-2) ...\n",
      "Setting up libevent-pthreads-2.1-7:amd64 (2.1.12-stable-1build3) ...\n",
      "Setting up libevent-extra-2.1-7:amd64 (2.1.12-stable-1build3) ...\n",
      "Setting up libtool (2.4.6-15build2) ...\n",
      "Setting up libgfortran-11-dev:amd64 (11.4.0-1ubuntu1~22.04) ...\n",
      "Setting up libevent-openssl-2.1-7:amd64 (2.1.12-stable-1build3) ...\n",
      "Setting up m4 (1.4.18-5ubuntu2) ...\n",
      "Setting up libhwloc-plugins:amd64 (2.7.0-2ubuntu1) ...\n",
      "Setting up python3.10-minimal (3.10.12-1~22.04.9) ...\n",
      "Setting up libnuma-dev:amd64 (2.0.14-3ubuntu2) ...\n",
      "Setting up libnl-route-3-200:amd64 (3.5.0-0.1) ...\n",
      "Setting up libpython3.10-stdlib:amd64 (3.10.12-1~22.04.9) ...\n",
      "Setting up libjs-jquery-ui (1.13.1+dfsg-1) ...\n",
      "Setting up libevent-dev (2.1.12-stable-1build3) ...\n",
      "Setting up libjs-sphinxdoc (4.3.2-1) ...\n",
      "Setting up gfortran-11 (11.4.0-1ubuntu1~22.04) ...\n",
      "Setting up autoconf (2.71-2) ...\n",
      "Setting up libnl-3-dev:amd64 (3.5.0-0.1) ...\n",
      "Setting up libpython3-stdlib:amd64 (3.10.6-1~22.04.1) ...\n",
      "Setting up automake (1:1.16.5-1.3) ...\n",
      "update-alternatives: using /usr/bin/automake-1.16 to provide /usr/bin/automake (automake) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/automake.1.gz because associated file /usr/share/man/man1/automake-1.16.1.gz (of link group automake) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/aclocal.1.gz because associated file /usr/share/man/man1/aclocal-1.16.1.gz (of link group automake) doesn't exist\n",
      "Setting up libpython3.10:amd64 (3.10.12-1~22.04.9) ...\n",
      "Setting up libibverbs1:amd64 (39.0-1) ...\n",
      "Setting up python3.10 (3.10.12-1~22.04.9) ...\n",
      "Setting up libpmix2:amd64 (4.1.2-2ubuntu1) ...\n",
      "Setting up ibverbs-providers:amd64 (39.0-1) ...\n",
      "Setting up python3 (3.10.6-1~22.04.1) ...\n",
      "Setting up python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n",
      "Setting up gfortran (4:11.2.0-1ubuntu1) ...\n",
      "update-alternatives: using /usr/bin/gfortran to provide /usr/bin/f95 (f95) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/f95.1.gz because associated file /usr/share/man/man1/gfortran.1.gz (of link group f95) doesn't exist\n",
      "update-alternatives: using /usr/bin/gfortran to provide /usr/bin/f77 (f77) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/f77.1.gz because associated file /usr/share/man/man1/gfortran.1.gz (of link group f77) doesn't exist\n",
      "Setting up libnl-route-3-dev:amd64 (3.5.0-0.1) ...\n",
      "Setting up libltdl-dev:amd64 (2.4.6-15build2) ...\n",
      "Setting up libpython3.10-dev:amd64 (3.10.12-1~22.04.9) ...\n",
      "Setting up python3.10-dev (3.10.12-1~22.04.9) ...\n",
      "Setting up libhwloc-dev:amd64 (2.7.0-2ubuntu1) ...\n",
      "Setting up libpmix-dev:amd64 (4.1.2-2ubuntu1) ...\n",
      "Setting up python3-pkg-resources (59.6.0-1.2ubuntu0.22.04.2) ...\n",
      "Setting up librdmacm1:amd64 (39.0-1) ...\n",
      "Setting up libucx0:amd64 (1.12.1~rc2-1) ...\n",
      "Setting up libpython3-dev:amd64 (3.10.6-1~22.04.1) ...\n",
      "Setting up python3.10-venv (3.10.12-1~22.04.9) ...\n",
      "Setting up python3-setuptools (59.6.0-1.2ubuntu0.22.04.2) ...\n",
      "Setting up libcoarrays-dev:amd64 (2.9.2-3) ...\n",
      "Setting up libibverbs-dev:amd64 (39.0-1) ...\n",
      "Setting up python3-dev (3.10.6-1~22.04.1) ...\n",
      "Setting up python3-pip (22.0.2+dfsg-1ubuntu0.5) ...\n",
      "Setting up libfabric1:amd64 (1.11.0-3) ...\n",
      "Setting up libopenmpi3:amd64 (4.1.2-2ubuntu1) ...\n",
      "Setting up libcaf-openmpi-3:amd64 (2.9.2-3) ...\n",
      "Setting up openmpi-bin (4.1.2-2ubuntu1) ...\n",
      "update-alternatives: using /usr/bin/mpirun.openmpi to provide /usr/bin/mpirun (mpirun) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/mpirun.1.gz because associated file /usr/share/man/man1/mpirun.openmpi.1.gz (of link group mpirun) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/mpiexec.1.gz because associated file /usr/share/man/man1/mpiexec.openmpi.1.gz (of link group mpirun) doesn't exist\n",
      "update-alternatives: using /usr/bin/mpicc.openmpi to provide /usr/bin/mpicc (mpi) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/mpicc.1.gz because associated file /usr/share/man/man1/mpicc.openmpi.1.gz (of link group mpi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/mpic++.1.gz because associated file /usr/share/man/man1/mpic++.openmpi.1.gz (of link group mpi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/mpicxx.1.gz because associated file /usr/share/man/man1/mpicxx.openmpi.1.gz (of link group mpi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/mpiCC.1.gz because associated file /usr/share/man/man1/mpiCC.openmpi.1.gz (of link group mpi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/mpif77.1.gz because associated file /usr/share/man/man1/mpif77.openmpi.1.gz (of link group mpi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/mpif90.1.gz because associated file /usr/share/man/man1/mpif90.openmpi.1.gz (of link group mpi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/mpifort.1.gz because associated file /usr/share/man/man1/mpifort.openmpi.1.gz (of link group mpi) doesn't exist\n",
      "Setting up libcoarrays-openmpi-dev:amd64 (2.9.2-3) ...\n",
      "update-alternatives: using /usr/lib/x86_64-linux-gnu/open-coarrays/openmpi/bin/caf to provide /usr/bin/caf.openmpi (caf-openmpi) in auto mode\n",
      "update-alternatives: using /usr/bin/caf.openmpi to provide /usr/bin/caf (caf) in auto mode\n",
      "Setting up libopenmpi-dev:amd64 (4.1.2-2ubuntu1) ...\n",
      "update-alternatives: using /usr/lib/x86_64-linux-gnu/openmpi/include to provide /usr/include/x86_64-linux-gnu/mpi (mpi-x86_64-linux-gnu) in auto mode\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
      "Collecting tensorrt-llm==0.17.0.post1\n",
      "  Downloading https://pypi.nvidia.com/tensorrt-llm/tensorrt_llm-0.17.0.post1-cp310-cp310-linux_x86_64.whl (1947.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 GB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0mm\n",
      "\u001b[?25hCollecting flashinfer@ git+https://github.com/flashinfer-ai/flashinfer.git@06309c4e (from tensorrt-llm==0.17.0.post1)\n",
      "  Cloning https://github.com/flashinfer-ai/flashinfer.git (to revision 06309c4e) to /tmp/pip-install-zxeef10g/flashinfer_c9291a599d354cabbacfd39f16e909a6\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/flashinfer-ai/flashinfer.git /tmp/pip-install-zxeef10g/flashinfer_c9291a599d354cabbacfd39f16e909a6\n",
      "\u001b[33m  WARNING: Did not find branch or tag '06309c4e', assuming revision or ref.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Running command git checkout -q 06309c4e\n",
      "  Resolved https://github.com/flashinfer-ai/flashinfer.git to commit 06309c4e\n",
      "  Running command git submodule update --init --recursive -q\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting s2wrapper@ git+https://github.com/bfshi/scaling_on_scales.git@60da2afe (from tensorrt-llm==0.17.0.post1)\n",
      "  Cloning https://github.com/bfshi/scaling_on_scales.git (to revision 60da2afe) to /tmp/pip-install-zxeef10g/s2wrapper_7e9734e4ceef4b35a7192e88abe6288b\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/bfshi/scaling_on_scales.git /tmp/pip-install-zxeef10g/s2wrapper_7e9734e4ceef4b35a7192e88abe6288b\n",
      "\u001b[33m  WARNING: Did not find branch or tag '60da2afe', assuming revision or ref.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Running command git checkout -q 60da2afe\n",
      "  Resolved https://github.com/bfshi/scaling_on_scales.git to commit 60da2afe\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting accelerate>=0.25.0 (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting build (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting colored (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading colored-2.3.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting cuda-python (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading cuda_python-12.8.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting diffusers>=0.27.0 (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading diffusers-0.33.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting lark (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting mpi4py (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading mpi4py-4.0.3.tar.gz (466 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m466.3/466.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from tensorrt-llm==0.17.0.post1) (1.24.1)\n",
      "Collecting onnx>=1.12.0 (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting onnx-graphsurgeon>=0.5.2 (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading https://pypi.nvidia.com/onnx-graphsurgeon/onnx_graphsurgeon-0.5.8-py2.py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting openai (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading openai-1.76.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting polygraphy (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading https://pypi.nvidia.com/polygraphy/polygraphy-0.49.20-py2.py3-none-any.whl (354 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.9/354.9 kB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorrt-llm==0.17.0.post1) (5.9.6)\n",
      "Collecting pynvml>=11.5.0 (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading pynvml-12.0.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pulp (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading pulp-3.1.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting pandas (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h5py==3.12.1 (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading h5py-3.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting StrEnum (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading StrEnum-0.4.15-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting sentencepiece>=0.1.99 (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tensorrt~=10.8.0 (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading https://pypi.nvidia.com/tensorrt/tensorrt-10.8.0.43.tar.gz (35 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torch<=2.6.0a0,>=2.5.1 (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from tensorrt-llm==0.17.0.post1) (0.16.0+cu118)\n",
      "Collecting nvidia-modelopt~=0.23.0 (from nvidia-modelopt[torch]~=0.23.0->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-modelopt/nvidia_modelopt-0.23.2-py3-none-manylinux2014_x86_64.whl (602 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m602.7/602.7 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12 (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-nccl-cu12/nvidia_nccl_cu12-2.26.2.post1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (291.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.7/291.7 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12 (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-cuda-nvrtc-cu12/nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers<4.48.0,>=4.47.0 (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic>=2.9.1 (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading pydantic-2.11.3-py3-none-any.whl.metadata (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pillow==10.3.0 (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading pillow-10.3.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from tensorrt-llm==0.17.0.post1) (0.41.3)\n",
      "Collecting optimum (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading optimum-1.24.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting evaluate (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from tensorrt-llm==0.17.0.post1) (1.3.0)\n",
      "Collecting click (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting click-option-group (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading click_option_group-0.5.7-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting aenum (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading aenum-3.1.15-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: pyzmq in /usr/local/lib/python3.10/dist-packages (from tensorrt-llm==0.17.0.post1) (24.0.1)\n",
      "Collecting fastapi==0.115.4 (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading fastapi-0.115.4-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting httpx (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorrt-llm==0.17.0.post1) (68.2.2)\n",
      "Collecting ordered-set (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading ordered_set-4.1.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting starlette<0.42.0,>=0.40.0 (from fastapi==0.115.4->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from fastapi==0.115.4->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.25.0->tensorrt-llm==0.17.0.post1) (23.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.25.0->tensorrt-llm==0.17.0.post1) (6.0.1)\n",
      "Collecting huggingface-hub>=0.21.0 (from accelerate>=0.25.0->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors>=0.4.3 (from accelerate>=0.25.0->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: importlib-metadata in /usr/lib/python3/dist-packages (from diffusers>=0.27.0->tensorrt-llm==0.17.0.post1) (4.6.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers>=0.27.0->tensorrt-llm==0.17.0.post1) (3.9.0)\n",
      "Collecting regex!=2019.12.17 (from diffusers>=0.27.0->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers>=0.27.0->tensorrt-llm==0.17.0.post1) (2.31.0)\n",
      "Collecting nvidia-modelopt-core==0.23.2 (from nvidia-modelopt~=0.23.0->nvidia-modelopt[torch]~=0.23.0->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-modelopt-core/nvidia_modelopt_core-0.23.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cloudpickle>=1.6.0 (from nvidia-modelopt~=0.23.0->nvidia-modelopt[torch]~=0.23.0->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting ninja (from nvidia-modelopt~=0.23.0->nvidia-modelopt[torch]~=0.23.0->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting rich (from nvidia-modelopt~=0.23.0->nvidia-modelopt[torch]~=0.23.0->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting scipy (from nvidia-modelopt~=0.23.0->nvidia-modelopt[torch]~=0.23.0->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm (from nvidia-modelopt~=0.23.0->nvidia-modelopt[torch]~=0.23.0->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torchprofile>=0.0.4 (from nvidia-modelopt[torch]~=0.23.0->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading torchprofile-0.0.4-py3-none-any.whl.metadata (303 bytes)\n",
      "Collecting protobuf>=3.20.2 (from onnx>=1.12.0->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading protobuf-6.31.0rc1-cp39-abi3-manylinux2014_x86_64.whl.metadata (596 bytes)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.9.1->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.1 (from pydantic>=2.9.1->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading pydantic_core-2.33.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic>=2.9.1->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting nvidia-ml-py<13.0.0a0,>=12.0.0 (from pynvml>=11.5.0->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading nvidia_ml_py-12.570.86-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting tensorrt_cu12==10.8.0.43 (from tensorrt~=10.8.0->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading https://pypi.nvidia.com/tensorrt-cu12/tensorrt_cu12-10.8.0.43.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tensorrt_cu12_libs==10.8.0.43 (from tensorrt_cu12==10.8.0.43->tensorrt~=10.8.0->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading https://pypi.nvidia.com/tensorrt-cu12-libs/tensorrt_cu12_libs-10.8.0.43-py2.py3-none-manylinux_2_28_x86_64.whl (3100.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 GB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorrt_cu12_bindings==10.8.0.43 (from tensorrt_cu12==10.8.0.43->tensorrt~=10.8.0->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading https://pypi.nvidia.com/tensorrt-cu12-bindings/tensorrt_cu12_bindings-10.8.0.43-cp310-none-manylinux_2_28_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m129.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12 (from tensorrt_cu12_libs==10.8.0.43->tensorrt_cu12==10.8.0.43->tensorrt~=10.8.0->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-cuda-runtime-cu12/nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<=2.6.0a0,>=2.5.1->tensorrt-llm==0.17.0.post1) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<=2.6.0a0,>=2.5.1->tensorrt-llm==0.17.0.post1) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<=2.6.0a0,>=2.5.1->tensorrt-llm==0.17.0.post1) (2023.4.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12 (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-cuda-nvrtc-cu12/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12 (from tensorrt_cu12_libs==10.8.0.43->tensorrt_cu12==10.8.0.43->tensorrt~=10.8.0->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-cuda-runtime-cu12/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<=2.6.0a0,>=2.5.1->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-cuda-cupti-cu12/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch<=2.6.0a0,>=2.5.1->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<=2.6.0a0,>=2.5.1->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-cublas-cu12/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.2.1.3 (from torch<=2.6.0a0,>=2.5.1->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-cufft-cu12/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147 (from torch<=2.6.0a0,>=2.5.1->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-curand-cu12/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch<=2.6.0a0,>=2.5.1->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-cusolver-cu12/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch<=2.6.0a0,>=2.5.1->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-cusparse-cu12/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12 (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-nccl-cu12/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.4.127 (from torch<=2.6.0a0,>=2.5.1->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-nvtx-cu12/nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch<=2.6.0a0,>=2.5.1->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-nvjitlink-cu12/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.1.0 (from torch<=2.6.0a0,>=2.5.1->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting sympy==1.13.1 (from torch<=2.6.0a0,>=2.5.1->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<4.48.0,>=4.47.0->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting pyproject_hooks (from build->tensorrt-llm==0.17.0.post1)\n",
      "  Using cached pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build->tensorrt-llm==0.17.0.post1) (2.0.1)\n",
      "Collecting cuda-bindings~=12.8.0 (from cuda-python->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading cuda_bindings-12.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting datasets>=2.0.0 (from evaluate->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting dill (from evaluate->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting xxhash (from evaluate->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from evaluate->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading multiprocess-0.70.18-py310-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->tensorrt-llm==0.17.0.post1) (4.0.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->tensorrt-llm==0.17.0.post1) (2022.12.7)\n",
      "Collecting httpcore==1.* (from httpx->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading httpcore-1.0.8-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->tensorrt-llm==0.17.0.post1) (3.4)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai->tensorrt-llm==0.17.0.post1) (1.7.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading jiter-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai->tensorrt-llm==0.17.0.post1) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorrt-llm==0.17.0.post1) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting einops (from s2wrapper@ git+https://github.com/bfshi/scaling_on_scales.git@60da2afe->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision (from tensorrt-llm==0.17.0.post1)\n",
      "  Downloading torchvision-0.22.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "  Downloading torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "  Downloading torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->tensorrt-llm==0.17.0.post1) (1.1.3)\n",
      "Collecting pyarrow>=15.0.0 (from datasets>=2.0.0->evaluate->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill (from evaluate->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting requests (from diffusers>=0.27.0->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting multiprocess (from evaluate->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting aiohttp (from datasets>=2.0.0->evaluate->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "INFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting huggingface-hub>=0.21.0 (from accelerate>=0.25.0->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading huggingface_hub-0.30.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.30.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.30.0rc3-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.30.0rc2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.30.0rc1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.30.0rc0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: pip is still looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading huggingface_hub-0.29.3rc0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.29.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.29.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.29.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.29.0rc7-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading huggingface_hub-0.29.0rc6-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.29.0rc5-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.29.0rc4-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.29.0rc3-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.29.0rc2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.29.0rc1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.29.0rc0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.28.0rc5-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.28.0rc4-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.28.0rc3-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.28.0rc2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.28.0rc1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.28.0rc0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting fsspec[http]>=2021.05.0 (from evaluate->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->tensorrt-llm==0.17.0.post1) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers>=0.27.0->tensorrt-llm==0.17.0.post1) (2.1.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers>=0.27.0->tensorrt-llm==0.17.0.post1) (1.26.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<=2.6.0a0,>=2.5.1->tensorrt-llm==0.17.0.post1) (2.1.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->nvidia-modelopt~=0.23.0->nvidia-modelopt[torch]~=0.23.0->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->nvidia-modelopt~=0.23.0->nvidia-modelopt[torch]~=0.23.0->tensorrt-llm==0.17.0.post1) (2.16.1)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets>=2.0.0->evaluate->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.0.0->evaluate->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets>=2.0.0->evaluate->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate->tensorrt-llm==0.17.0.post1) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.0.0->evaluate->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.0.0->evaluate->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets>=2.0.0->evaluate->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets>=2.0.0->evaluate->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.4/72.4 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->nvidia-modelopt~=0.23.0->nvidia-modelopt[torch]~=0.23.0->tensorrt-llm==0.17.0.post1)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading fastapi-0.115.4-py3-none-any.whl (94 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading diffusers-0.33.1-py3-none-any.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.11.3-py3-none-any.whl (443 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.6/443.6 kB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.33.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pynvml-12.0.0-py3-none-any.whl (26 kB)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hDownloading aenum-3.1.15-py3-none-any.whl (137 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click_option_group-0.5.7-py3-none-any.whl (11 kB)\n",
      "Downloading colored-2.3.0-py3-none-any.whl (18 kB)\n",
      "Downloading cuda_python-12.8.0-py3-none-any.whl (11 kB)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.8-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.76.0-py3-none-any.whl (661 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.2/661.2 kB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading optimum-1.24.0-py3-none-any.whl (433 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.6/433.6 kB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
      "Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pulp-3.1.1-py3-none-any.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading StrEnum-0.4.15-py3-none-any.whl (8.9 kB)\n",
      "Downloading torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading cuda_bindings-12.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.4/481.4 kB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.9/352.9 kB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_ml_py-12.570.86-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.31.0rc1-cp39-abi3-manylinux2014_x86_64.whl (320 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.0/321.0 kB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchprofile-0.0.4-py3-none-any.whl (7.7 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.3/287.3 kB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (219 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.8/219.8 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.6/206.6 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (333 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.9/333.9 kB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: tensorrt, tensorrt_cu12, flashinfer, mpi4py, s2wrapper\n",
      "  Building wheel for tensorrt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tensorrt: filename=tensorrt-10.8.0.43-py2.py3-none-any.whl size=42251 sha256=fd744e27853db0a547b76ef37c1bc716f1fa004bedd377c0b613c540d378dc14\n",
      "  Stored in directory: /root/.cache/pip/wheels/da/d8/6b/a2f9e442aff33a38635b5ec917646ada657125482a70e780b1\n",
      "  Building wheel for tensorrt_cu12 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tensorrt_cu12: filename=tensorrt_cu12-10.8.0.43-py2.py3-none-any.whl size=17614 sha256=5bc33a529469e4d2dab33f2aadb9f3f4faa1c6673763d52814a3779a5c43870b\n",
      "  Stored in directory: /root/.cache/pip/wheels/b0/39/59/1c551050a671dc869f0d4ed2618cf6b64718321d83d328c657\n",
      "  Building wheel for flashinfer (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for flashinfer: filename=flashinfer-0.2.0.post1-py3-none-any.whl size=3232474 sha256=cd18e019952046b748896f73c1799a07bcdac6fb70a650f66be3b5671ede55d1\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-i_8wmuos/wheels/2d/e6/23/5161dbe09ed2440b9fe1491f857494a5c6c7787246bdd24853\n",
      "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for mpi4py: filename=mpi4py-4.0.3-cp310-cp310-linux_x86_64.whl size=4249164 sha256=c63bb3bf9d0ba77e9ff03f48f153e453c11bd89c3d8b1c3e7eea589c7ced244a\n",
      "  Stored in directory: /root/.cache/pip/wheels/94/57/cc/2e34c76a8690ce2a90b200702473f579b23f8224b5058b6a6d\n",
      "  Building wheel for s2wrapper (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for s2wrapper: filename=s2wrapper-0.1-py3-none-any.whl size=8643 sha256=d0603c06fceb894d8197ce7c904d3188925c5599420cc67532c4b0121217ace6\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-i_8wmuos/wheels/9f/43/2b/2a06f9c7871608fd2f4d30b67ddcd60e59525a692a42bf73c8\n",
      "Successfully built tensorrt tensorrt_cu12 flashinfer mpi4py s2wrapper\n",
      "Installing collected packages: tensorrt_cu12_bindings, StrEnum, sentencepiece, pytz, nvidia-ml-py, cuda-bindings, aenum, xxhash, tzdata, typing-extensions, triton, tqdm, sympy, scipy, safetensors, requests, regex, pyproject_hooks, pynvml, pyarrow, pulp, protobuf, propcache, polygraphy, pillow, ordered-set, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-modelopt-core, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ninja, mpi4py, mdurl, lark, jiter, h5py, h11, fsspec, frozenlist, einops, dill, cuda-python, colored, cloudpickle, click, async-timeout, annotated-types, aiohappyeyeballs, uvicorn, typing-inspection, tensorrt_cu12_libs, starlette, s2wrapper, pydantic-core, pandas, onnx, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, multidict, markdown-it-py, huggingface-hub, httpcore, click-option-group, build, aiosignal, yarl, tokenizers, tensorrt_cu12, rich, pydantic, onnx-graphsurgeon, nvidia-cusolver-cu12, httpx, diffusers, transformers, torch, tensorrt, openai, nvidia-modelopt, fastapi, aiohttp, torchvision, optimum, flashinfer, accelerate, torchprofile, datasets, evaluate, tensorrt-llm\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.1.0\n",
      "    Uninstalling triton-2.1.0:\n",
      "      Successfully uninstalled triton-2.1.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12\n",
      "    Uninstalling sympy-1.12:\n",
      "      Successfully uninstalled sympy-1.12\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 9.3.0\n",
      "    Uninstalling Pillow-9.3.0:\n",
      "      Successfully uninstalled Pillow-9.3.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.0+cu118\n",
      "    Uninstalling torch-2.1.0+cu118:\n",
      "      Successfully uninstalled torch-2.1.0+cu118\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.16.0+cu118\n",
      "    Uninstalling torchvision-0.16.0+cu118:\n",
      "      Successfully uninstalled torchvision-0.16.0+cu118\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed StrEnum-0.4.15 accelerate-1.6.0 aenum-3.1.15 aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 annotated-types-0.7.0 async-timeout-5.0.1 build-1.2.2.post1 click-8.1.8 click-option-group-0.5.7 cloudpickle-3.1.1 colored-2.3.0 cuda-bindings-12.8.0 cuda-python-12.8.0 datasets-3.5.0 diffusers-0.33.1 dill-0.3.8 einops-0.8.1 evaluate-0.4.3 fastapi-0.115.4 flashinfer-0.2.0.post1 frozenlist-1.6.0 fsspec-2024.12.0 h11-0.14.0 h5py-3.12.1 httpcore-1.0.8 httpx-0.28.1 huggingface-hub-0.30.2 jiter-0.9.0 lark-1.2.2 markdown-it-py-3.0.0 mdurl-0.1.2 mpi4py-4.0.3 multidict-6.4.3 multiprocess-0.70.16 ninja-1.11.1.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-ml-py-12.570.86 nvidia-modelopt-0.23.2 nvidia-modelopt-core-0.23.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 onnx-1.17.0 onnx-graphsurgeon-0.5.8 openai-1.76.0 optimum-1.24.0 ordered-set-4.1.0 pandas-2.2.3 pillow-10.3.0 polygraphy-0.49.20 propcache-0.3.1 protobuf-6.31.0rc1 pulp-3.1.1 pyarrow-19.0.1 pydantic-2.11.3 pydantic-core-2.33.1 pynvml-12.0.0 pyproject_hooks-1.2.0 pytz-2025.2 regex-2024.11.6 requests-2.32.3 rich-14.0.0 s2wrapper-0.1 safetensors-0.5.3 scipy-1.15.2 sentencepiece-0.2.0 starlette-0.41.3 sympy-1.13.1 tensorrt-10.8.0.43 tensorrt-llm-0.17.0.post1 tensorrt_cu12-10.8.0.43 tensorrt_cu12_bindings-10.8.0.43 tensorrt_cu12_libs-10.8.0.43 tokenizers-0.21.1 torch-2.5.1 torchprofile-0.0.4 torchvision-0.20.1 tqdm-4.67.1 transformers-4.47.1 triton-3.1.0 typing-extensions-4.13.2 typing-inspection-0.4.0 tzdata-2025.2 uvicorn-0.34.2 xxhash-3.5.0 yarl-1.20.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!apt-get update && apt-get -y install python3.10 python3-pip openmpi-bin libopenmpi-dev\n",
    "!pip3 install tensorrt-llm==0.17.0.post1 --pre --extra-index-url https://pypi.nvidia.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d029d061-72a2-4a9d-979e-0036324b4220",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Downloading BLOOM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846f303e-ef4d-42d0-8cd6-a7ec1e7ff677",
   "metadata": {},
   "source": [
    "BLOOM is an autoregressive Large Language Model (LLM), trained to continue text from a prompt on vast amounts of text data using industrial-scale computational resources. As such, it is able to output coherent text in 46 languages and 13 programming languages that is hardly distinguishable from text written by humans. BLOOM can also be instructed to perform text tasks it hasn't been explicitly trained for, by casting them as text generation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dbc0ca-1cdb-46dc-82b2-c3e23ba9b18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r TensorRT-LLM/examples/bloom/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cf59807",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  git-lfs\n",
      "0 upgraded, 1 newly installed, 0 to remove and 131 not upgraded.\n",
      "Need to get 3544 kB of archives.\n",
      "After this operation, 10.5 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 git-lfs amd64 3.0.2-1ubuntu0.3 [3544 kB]\n",
      "Fetched 3544 kB in 1s (6614 kB/s)\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package git-lfs.\n",
      "(Reading database ... 25341 files and directories currently installed.)\n",
      "Preparing to unpack .../git-lfs_3.0.2-1ubuntu0.3_amd64.deb ...\n",
      "Unpacking git-lfs (3.0.2-1ubuntu0.3) ...\n",
      "Setting up git-lfs (3.0.2-1ubuntu0.3) ...\n",
      "Git LFS initialized.\n"
     ]
    }
   ],
   "source": [
    "!apt-get install git-lfs\n",
    "!git lfs install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f9c485-9bbc-4dea-9d2d-240ac8f5ced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOOM_PATH=\"TensorRT-LLM/examples/bloom\"\n",
    "\n",
    "!rm -rf TensorRT-LLM/examples/bloom/560M\n",
    "!mkdir -p TensorRT-LLM/examples/bloom/560M && git lfs clone https://huggingface.co/bigscience/bloom-560m TensorRT-LLM/examples/bloom/560M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ab5234-a53e-4bdf-beb6-807dff0df9db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Converting and Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c25e4c3a-94a4-40d7-8046-80b6cdfd8cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorRT-LLM] TensorRT-LLM version: 0.9.0.dev2024030500\n",
      "0.9.0.dev2024030500\n",
      "[03/06/2024-20:31:52] [TRT-LLM] [I] Convert by using checkpoint\n",
      "[03/06/2024-20:31:52] [TRT-LLM] [I] Loading weights from HF BLOOM...\n",
      "Rank [0] Loading weights: 100%|███████████████████| 1/1 [00:05<00:00,  5.40s/it]\n",
      "[03/06/2024-20:31:58] [TRT-LLM] [I] Weights loaded. Total time: 00:00:05\n",
      "Total time of converting checkpoints: 00:00:26\n"
     ]
    }
   ],
   "source": [
    "# Convert checkpoint from HF to TRT-LLM format\n",
    "!python3 TensorRT-LLM/examples/bloom/convert_checkpoint.py --model_dir TensorRT-LLM/examples/bloom/560M/ \\\n",
    "                --dtype float16 \\\n",
    "                --output_dir TensorRT-LLM/examples/bloom/560M/trt_ckpt/fp16/1-gpu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653d0c5c-63ab-43ae-9378-5247b08dc4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build TensorRT-LLM model from checkpoint\n",
    "!trtllm-build --checkpoint_dir TensorRT-LLM/examples/bloom/560M/trt_ckpt/fp16/1-gpu/ \\\n",
    "                --gemm_plugin float16 \\\n",
    "                --output_dir TensorRT-LLM/examples/bloom/560M/trt_engines/fp16/1-gpu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beacb68b-10e3-4a60-b190-f9e0dd423dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the BLOOM 560M using a single GPU and apply INT8 weight-only quantization.\n",
    "!python3 TensorRT-LLM/examples/bloom/convert_checkpoint.py --model_dir TensorRT-LLM/examples/bloom/560M \\\n",
    "                --dtype float16 \\\n",
    "                --use_weight_only \\\n",
    "                --output_dir TensorRT-LLM/examples/bloom/560M/trt_ckpt/int8_weight_only/1-gpu/\n",
    "\n",
    "!trtllm-build --checkpoint_dir TensorRT-LLM/examples/bloom/560M/trt_ckpt/int8_weight_only/1-gpu/ \\\n",
    "                --gemm_plugin float16 \\\n",
    "                --output_dir TensorRT-LLM/examples/bloom/560M/trt_engines/int8_weight_only/1-gpu/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d19e11-3181-43af-ac1e-1d9a3ce99aab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1bb3e0-67f1-4e61-b60b-bf329b273f9e",
   "metadata": {},
   "source": [
    "Comparing execution time and **ROUGE** metrics on a **Summarization Task**.\n",
    "\n",
    "**ROUGE** stands for **Recall-Oriented Understudy for Gisting Evaluation**.<br />\n",
    "It is a set of metrics used to evaluate the quality of summaries by comparing them to one or more reference summaries.<br />\n",
    "\n",
    "Since we are using NVIDIA's Python scripts for benchmarking, we need to capture **STDOUT** and parse the results from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0b28c0ad-eaa6-42cc-a9d1-785e60936d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture bloom_hf_results\n",
    "# Huggingface\n",
    "!time python3 TensorRT-LLM/examples/bloom/../summarize.py --test_hf \\\n",
    "                       --hf_model_dir TensorRT-LLM/examples/bloom/560M/ \\\n",
    "                       --data_type fp16 \\\n",
    "                       --engine_dir TensorRT-LLM/examples/bloom/560M/trt_engines/fp16/1-gpu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "85de27d0-0d75-4ef1-a0da-5e17608e99fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture bloom_trt_results\n",
    "# TensorRT-LLM\n",
    "!time python3 TensorRT-LLM/examples/bloom/../summarize.py --test_trt_llm \\\n",
    "                       --hf_model_dir TensorRT-LLM/examples/bloom/560M/ \\\n",
    "                       --data_type fp16 \\\n",
    "                       --engine_dir TensorRT-LLM/examples/bloom/560M/trt_engines/fp16/1-gpu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "14d8d640-cd32-48f1-8d46-612871d53ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture bloom_int8_results\n",
    "# TensorRT-LLM (INT8)\n",
    "!time python3 TensorRT-LLM/examples/bloom/../summarize.py --test_trt_llm \\\n",
    "                       --hf_model_dir TensorRT-LLM/examples/bloom/560M/ \\\n",
    "                       --data_type fp16 \\\n",
    "                       --engine_dir TensorRT-LLM/examples/bloom/560M/trt_engines/int8_weight_only/1-gpu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37fa8c1-83b4-49cf-861e-20a382625faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example output captured\n",
    "print(bloom_int8_results())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54300987-e179-4b03-bb3c-39e9c2a80299",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Visualizing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56649eab-8b19-410a-8a73-573b13d553bf",
   "metadata": {},
   "source": [
    "#### Parse results from STDOUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "31223187-e3e7-4d1c-a867-d7ca4ebaa500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_output(output):\n",
    "    # Extracting execution time\n",
    "    time_match = re.search(r'real\\s+(\\d+)m([\\d.]+)s', output.stdout)\n",
    "    if time_match:\n",
    "        minutes = int(time_match.group(1))\n",
    "        seconds = float(time_match.group(2))\n",
    "        exec_time = minutes * 60 + seconds\n",
    "    else:\n",
    "        exec_time = None\n",
    "    \n",
    "    rouge_scores = re.findall(r'rouge[12Lsum]+ : ([\\d.]+)', output.stdout)\n",
    "    rouge_scores = [float(score) for score in rouge_scores] if rouge_scores else []\n",
    "    \n",
    "    latency_match = re.search(r'total latency: ([\\d.]+) sec', output.stdout)\n",
    "    latency = float(latency_match.group(1)) if latency_match else None\n",
    "    \n",
    "    tokens_match = re.search(r'total output tokens: (\\d+)', output.stdout)\n",
    "    total_tokens = int(tokens_match.group(1)) if tokens_match else None\n",
    "    \n",
    "    tokens_per_sec_match = re.search(r'tokens per second: ([\\d.]+)', output.stdout)\n",
    "    tokens_per_sec = float(tokens_per_sec_match.group(1)) if tokens_per_sec_match else None\n",
    "    \n",
    "    return exec_time, rouge_scores, latency, total_tokens, tokens_per_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0d11d69a-36cf-4568-bcee-cd703823d2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For bloom_hf_results\n",
    "bloom_hf_exec_time, bloom_hf_rouge, _, _, _ = parse_output(bloom_hf_results)\n",
    "\n",
    "# Only TensorRT-LLM optimized models have Tokens/s, latency, and total tokens\n",
    "bloom_trt_exec_time, bloom_trt_rouge, bloom_trt_latency, bloom_trt_tokens, bloom_trt_tokens_per_sec = parse_output(bloom_trt_results)\n",
    "bloom_int8_exec_time, bloom_int8_rouge, bloom_int8_latency, bloom_int8_tokens, bloom_int8_tokens_per_sec = parse_output(bloom_int8_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a7429695-0d60-4dbd-bb54-21f3b88ae27f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Model')"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Execution times and Rouge Scores\n",
    "exec_times = [bloom_hf_exec_time, bloom_trt_exec_time, bloom_int8_exec_time]\n",
    "models = ['HF', 'TRT', 'INT8']\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plotting execution times\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.bar(models, exec_times, color=['blue', 'green', 'red'])\n",
    "plt.title('Time Comparison (lower is better)')\n",
    "plt.ylabel('Execution Time (seconds)')\n",
    "plt.xlabel('Model')\n",
    "\n",
    "# Plotting ROUGE-1 scores\n",
    "rouge1_scores = [bloom_hf_rouge[0], bloom_trt_rouge[0], bloom_int8_rouge[0]]\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.bar(models, rouge1_scores, color=['blue', 'green', 'red'])\n",
    "plt.title('ROUGE-1 (higher is better)')\n",
    "plt.ylabel('ROUGE-1 Score')\n",
    "plt.xlabel('Model')\n",
    "\n",
    "# Plotting ROUGE-Lsum scores\n",
    "rouge_lsum_scores = [bloom_hf_rouge[3], bloom_trt_rouge[3], bloom_int8_rouge[3]]\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar(models, rouge_lsum_scores, color=['blue', 'green', 'red'])\n",
    "plt.title('ROUGE-Lsum (higher is better)')\n",
    "plt.ylabel('ROUGE-Lsum Score')\n",
    "plt.xlabel('Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ecafe57c-3351-4e87-b0d2-6f78851e4900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Model')"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Latency and Tokens per Sec\n",
    "latencies = [bloom_trt_latency, bloom_int8_latency]\n",
    "tps = [bloom_trt_tokens_per_sec, bloom_int8_tokens_per_sec]\n",
    "models = ['TRT', 'INT8']\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plotting latency times\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(models, latencies, color=['green', 'red'])\n",
    "plt.title('Latency (lower is better)')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.xlabel('Model')\n",
    "\n",
    "# Plotting tokens per seconds\n",
    "rouge1_scores = tps\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(models, rouge1_scores, color=['green', 'red'])\n",
    "plt.title('Tokens per second (higher is better)')\n",
    "plt.ylabel('Tokens/sec')\n",
    "plt.xlabel('Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d4d4b5",
   "metadata": {},
   "source": [
    "# Launch Qwen2.5 1.5B model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0bf2ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: 'git lfs clone' is deprecated and will not be updated\n",
      "          with new flags from 'git clone'\n",
      "\n",
      "'git clone' has been updated in upstream Git to have comparable\n",
      "speeds to 'git lfs clone'.\n",
      "Cloning into 'TensorRT-LLM/examples/qwen/7B'...\n",
      "remote: Enumerating objects: 50, done.\u001b[K\n",
      "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
      "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
      "remote: Total 50 (delta 21), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
      "Unpacking objects: 100% (50/50), 3.61 MiB | 2.10 MiB/s, done.\n",
      "Downloading LFS objects: 100% (4/4), 15 GB | 242 MB/s                           \r"
     ]
    }
   ],
   "source": [
    "!mkdir -p TensorRT-LLM/examples/qwen/7B && git lfs clone https://huggingface.co/Qwen/Qwen2.5-7B-Instruct TensorRT-LLM/examples/qwen/7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c15abdf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorRT-LLM] TensorRT-LLM version: 0.17.0.post1\n",
      "0.17.0.post1\n",
      "230it [00:27,  8.23it/s]\n",
      "Total time of converting checkpoints: 00:01:18\n"
     ]
    }
   ],
   "source": [
    "# Convert checkpoint from HF to TRT-LLM format\n",
    "!python3 TensorRT-LLM/examples/qwen/convert_checkpoint.py --model_dir TensorRT-LLM/examples/qwen/7B/ \\\n",
    "                --dtype float16 \\\n",
    "                --output_dir TensorRT-LLM/examples/qwen/7B/trt_ckpt/fp16/1-gpu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddd97f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorRT-LLM] TensorRT-LLM version: 0.17.0.post1\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] Set bert_attention_plugin to auto.\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] Set gpt_attention_plugin to auto.\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] Set gemm_plugin to float16.\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] Set gemm_swiglu_plugin to None.\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] Set fp8_rowwise_gemm_plugin to None.\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] Set nccl_plugin to auto.\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] Set lora_plugin to None.\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] Set moe_plugin to auto.\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] Set mamba_conv1d_plugin to auto.\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] Set low_latency_gemm_plugin to None.\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] Set low_latency_gemm_swiglu_plugin to None.\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] Set context_fmha to True.\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] Set bert_context_fmha_fp32_acc to False.\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] Set remove_input_padding to True.\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] Set reduce_fusion to False.\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] Set user_buffer to False.\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] Set tokens_per_block to 64.\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] Set use_paged_context_fmha to False.\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] Set use_fp8_context_fmha to False.\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] Set multiple_profiles to False.\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] Set paged_state to True.\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] Set streamingllm to False.\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] Set use_fused_mlp to True.\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] Set pp_reduce_scatter to False.\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [W] Implicitly setting QWenConfig.seq_length = 8192\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [W] Implicitly setting QWenConfig.qwen_type = qwen2\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [W] Implicitly setting QWenConfig.moe_intermediate_size = 0\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [W] Implicitly setting QWenConfig.moe_shared_expert_intermediate_size = 0\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [W] Implicitly setting QWenConfig.tie_word_embeddings = False\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] Compute capability: (8, 6)\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] SM count: 56\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] SM clock: 2100 MHz\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] int4 TFLOPS: 481\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] int8 TFLOPS: 240\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] fp8 TFLOPS: 0\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] float16 TFLOPS: 120\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] bfloat16 TFLOPS: 120\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] float32 TFLOPS: 60\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] Total Memory: 19 GiB\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] Memory clock: 8001 MHz\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] Memory bus width: 320\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] Memory bandwidth: 640 GB/s\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] NVLink is active: False\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] PCIe speed: 2500 Mbps\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] PCIe link width: 16\n",
      "[04/24/2025-15:31:58] [TRT-LLM] [I] PCIe bandwidth: 5 GB/s\n",
      "[04/24/2025-15:32:02] [TRT-LLM] [W] Provided but not required tensors: {'embed_positions_for_gpt_attention', 'rotary_inv_freq', 'embed_positions'}\n",
      "[04/24/2025-15:32:02] [TRT-LLM] [I] Set dtype to float16.\n",
      "[04/24/2025-15:32:02] [TRT-LLM] [I] Set paged_kv_cache to True.\n",
      "[04/24/2025-15:32:02] [TRT-LLM] [W] Overriding paged_state to False\n",
      "[04/24/2025-15:32:02] [TRT-LLM] [I] Set paged_state to False.\n",
      "[04/24/2025-15:32:02] [TRT-LLM] [W] remove_input_padding is enabled, while opt_num_tokens is not set, setting to max_batch_size*max_beam_width. \n",
      "\n",
      "[04/24/2025-15:32:02] [TRT-LLM] [W] max_num_tokens (4096) shouldn't be greater than max_seq_len * max_batch_size (4096), specifying to max_seq_len * max_batch_size (4096).\n",
      "[04/24/2025-15:32:02] [TRT-LLM] [W] padding removal and fMHA are both enabled, max_input_len is not required and will be ignored\n",
      "[04/24/2025-15:32:29] [TRT] [I] [MemUsageChange] Init CUDA: CPU -15, GPU +0, now: CPU 7569, GPU 189 (MiB)\n",
      "[04/24/2025-15:32:33] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +2648, GPU +412, now: CPU 10287, GPU 601 (MiB)\n",
      "[04/24/2025-15:32:33] [TRT-LLM] [I] Set nccl_plugin to None.\n",
      "[04/24/2025-15:32:33] [TRT-LLM] [I] Total time of constructing network from module object 31.62024712562561 seconds\n",
      "[04/24/2025-15:32:33] [TRT-LLM] [I] Total optimization profiles added: 1\n",
      "[04/24/2025-15:32:33] [TRT-LLM] [I] Total time to initialize the weights in network Unnamed Network 0: 00:00:00\n",
      "[04/24/2025-15:32:33] [TRT-LLM] [I] Build TensorRT engine Unnamed Network 0\n",
      "[04/24/2025-15:32:33] [TRT] [W] Unused Input: position_ids\n",
      "[04/24/2025-15:32:33] [TRT] [W] [RemoveDeadLayers] Input Tensor position_ids is unused or used only at compile-time, but is not being removed.\n",
      "[04/24/2025-15:32:33] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.\n",
      "[04/24/2025-15:32:33] [TRT] [I] Compiler backend is used during engine build.\n",
      "[04/24/2025-15:32:37] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n",
      "[04/24/2025-15:32:37] [TRT] [I] Detected 17 inputs and 1 output network tensors.\n",
      "[04/24/2025-15:32:58] [TRT] [I] Total Host Persistent Memory: 89968 bytes\n",
      "[04/24/2025-15:32:58] [TRT] [I] Total Device Persistent Memory: 0 bytes\n",
      "[04/24/2025-15:32:58] [TRT] [I] Max Scratch Memory: 103369728 bytes\n",
      "[04/24/2025-15:32:58] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 466 steps to complete.\n",
      "[04/24/2025-15:32:58] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 29.6934ms to assign 20 blocks to 466 nodes requiring 583015424 bytes.\n",
      "[04/24/2025-15:32:58] [TRT] [I] Total Activation Memory: 583013888 bytes\n",
      "[04/24/2025-15:32:59] [TRT] [I] Total Weights Memory: 15248010880 bytes\n",
      "[04/24/2025-15:32:59] [TRT] [I] Compiler backend is used during engine execution.\n",
      "[04/24/2025-15:32:59] [TRT] [I] Engine generation completed in 25.2406 seconds.\n",
      "[04/24/2025-15:32:59] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 14541 MiB\n",
      "[04/24/2025-15:33:10] [TRT-LLM] [I] Total time of building Unnamed Network 0: 00:00:36\n",
      "[04/24/2025-15:33:10] [TRT] [I] Serialized 27 bytes of code generator cache.\n",
      "[04/24/2025-15:33:10] [TRT] [I] Serialized 217351 bytes of compilation cache.\n",
      "[04/24/2025-15:33:10] [TRT] [I] Serialized 12 timing cache entries\n",
      "[04/24/2025-15:33:10] [TRT-LLM] [I] Timing cache serialized to model.cache\n",
      "[04/24/2025-15:33:10] [TRT-LLM] [I] Build phase peak memory: 42122.47 MB, children: 24.59 MB\n",
      "[04/24/2025-15:33:10] [TRT-LLM] [I] Serializing engine to TensorRT-LLM/examples/qwen/7B/trt_engines/fp16/1-gpu/rank0.engine...\n",
      "[04/24/2025-15:33:43] [TRT-LLM] [I] Engine serialized. Total time: 00:00:32\n",
      "[04/24/2025-15:33:45] [TRT-LLM] [I] Total time of building all engines: 00:01:46\n"
     ]
    }
   ],
   "source": [
    "# Build TensorRT-LLM model from checkpoint\n",
    "!trtllm-build --checkpoint_dir TensorRT-LLM/examples/qwen/7B/trt_ckpt/fp16/1-gpu/ \\\n",
    "                --max_seq_len 4096 \\\n",
    "                --max_num_tokens 4096 \\\n",
    "                --max_batch_size 1 \\\n",
    "                --gemm_plugin float16 \\\n",
    "                --output_dir TensorRT-LLM/examples/qwen/7B/trt_engines/fp16/1-gpu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59994b3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorRT-LLM] TensorRT-LLM version: 0.17.0.post1\n",
      "[TensorRT-LLM][INFO] Engine version 0.17.0.post1 found in the config file, assuming engine(s) built by new builder API.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Using C++ session\n",
      "[TensorRT-LLM][INFO] Engine version 0.17.0.post1 found in the config file, assuming engine(s) built by new builder API.\n",
      "[TensorRT-LLM][INFO] MPI size: 1, MPI local size: 1, rank: 0\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [W] Implicitly setting QWenConfig.seq_length = 8192\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [W] Implicitly setting QWenConfig.qwen_type = qwen2\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [W] Implicitly setting QWenConfig.moe_intermediate_size = 0\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [W] Implicitly setting QWenConfig.moe_shared_expert_intermediate_size = 0\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [W] Implicitly setting QWenConfig.tie_word_embeddings = False\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set dtype to float16.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set bert_attention_plugin to auto.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set gpt_attention_plugin to auto.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set gemm_plugin to float16.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set explicitly_disable_gemm_plugin to False.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set gemm_swiglu_plugin to None.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set fp8_rowwise_gemm_plugin to None.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set qserve_gemm_plugin to None.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set identity_plugin to None.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set nccl_plugin to None.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set lora_plugin to None.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set weight_only_groupwise_quant_matmul_plugin to None.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set weight_only_quant_matmul_plugin to None.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set smooth_quant_plugins to True.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set smooth_quant_gemm_plugin to None.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set layernorm_quantization_plugin to None.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set rmsnorm_quantization_plugin to None.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set quantize_per_token_plugin to False.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set quantize_tensor_plugin to False.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set moe_plugin to auto.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set mamba_conv1d_plugin to auto.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set low_latency_gemm_plugin to None.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set low_latency_gemm_swiglu_plugin to None.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set context_fmha to True.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set bert_context_fmha_fp32_acc to False.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set paged_kv_cache to True.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set remove_input_padding to True.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set reduce_fusion to False.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set user_buffer to False.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set tokens_per_block to 64.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set use_paged_context_fmha to False.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set use_fp8_context_fmha to False.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set multiple_profiles to False.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set paged_state to False.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set streamingllm to False.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set manage_weights to False.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set use_fused_mlp to True.\n",
      "[04/24/2025-15:24:30] [TRT-LLM] [I] Set pp_reduce_scatter to False.\n",
      "[TensorRT-LLM][INFO] Engine version 0.17.0.post1 found in the config file, assuming engine(s) built by new builder API.\n",
      "[TensorRT-LLM][INFO] Refreshed the MPI local session\n",
      "[TensorRT-LLM][INFO] MPI size: 1, MPI local size: 1, rank: 0\n",
      "[TensorRT-LLM][INFO] Rank 0 is using GPU 0\n",
      "[TensorRT-LLM][WARNING] Fix optionalParams : KV cache reuse disabled because model was not built with paged context FMHA support\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxNumSequences: 1\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxBatchSize: 1\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxBeamWidth: 1\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxSequenceLen: 4096\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxDraftLen: 0\n",
      "[TensorRT-LLM][INFO] TRTGptModel mMaxAttentionWindowSize: (4096) * 28\n",
      "[TensorRT-LLM][INFO] TRTGptModel enableTrtOverlap: 0\n",
      "[TensorRT-LLM][INFO] TRTGptModel normalizeLogProbs: 1\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxNumTokens: 8192\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxInputLen: 4095 = min(maxSequenceLen - 1, maxNumTokens) since context FMHA and usePackedInput are enabled\n",
      "[TensorRT-LLM][INFO] TRTGptModel If model type is encoder, maxInputLen would be reset in trtEncoderModel to maxInputLen: min(maxSequenceLen, maxNumTokens).\n",
      "[TensorRT-LLM][INFO] Capacity Scheduler Policy: GUARANTEED_NO_EVICT\n",
      "[TensorRT-LLM][INFO] Context Chunking Scheduler Policy: None\n",
      "[TensorRT-LLM][INFO] Loaded engine size: 14548 MiB\n",
      "[TensorRT-LLM][INFO] Inspecting the engine to identify potential runtime issues...\n",
      "[TensorRT-LLM][INFO] The profiling verbosity of the engine does not allow this analysis to proceed. Re-build the engine with 'detailed' profiling verbosity to get more diagnostics.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 1016.01 MiB for execution context memory.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 14541 (MiB)\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 722.07 KB GPU memory for runtime buffers.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 2.45 MB GPU memory for decoder.\n",
      "[TensorRT-LLM][INFO] Memory usage when calculating max tokens in paged kv cache: total: 19.60 GiB, available: 4.16 GiB\n",
      "[TensorRT-LLM][INFO] Number of blocks in KV cache primary pool: 1097\n",
      "[TensorRT-LLM][INFO] Number of blocks in KV cache secondary pool: 0, onboard blocks to primary memory before reuse: true\n",
      "[TensorRT-LLM][INFO] KV cache block reuse is disabled\n",
      "[TensorRT-LLM][INFO] Max KV cache pages per sequence: 64\n",
      "[TensorRT-LLM][INFO] Number of tokens per block: 64.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 3.75 GiB for max tokens in paged KV cache (70208).\n",
      "[04/24/2025-15:25:05] [TRT-LLM] [I] Load engine takes: 34.655555725097656 sec\n",
      "Input [Text 0]: \"<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "How do I count to nine in French?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\"\n",
      "Output [Text 0 Beam 0]: \"Counting to nine in French is quite straightforward:\n",
      "\n",
      "1. Un\n",
      "2. Deux\n",
      "3. Trois\n",
      "4. Quatre\n",
      "5. Cinq\n",
      "6. Six\n",
      "7. Sept\n",
      "8. Huit\n",
      "9. Neuf\n",
      "\n",
      "So, to count to nine in French, you would say: \"Un, deux, trois, quatre, cinq, six, sept, huit, neuf.\"\"\n",
      "[TensorRT-LLM][INFO] Refreshed the MPI local session\n"
     ]
    }
   ],
   "source": [
    "!python3 TensorRT-LLM/examples/run.py --engine_dir TensorRT-LLM/examples/qwen/7B/trt_engines/fp16/1-gpu/  --max_output_len 100 --tokenizer_dir TensorRT-LLM/examples/qwen/7B/ --input_text \"How do I count to nine in French?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdbb76ee-0745-4e8a-8653-7b4583775db6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-24 13:49:50,531 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
      "[TensorRT-LLM] TensorRT-LLM version: 0.20.0rc0\n",
      "[TensorRT-LLM][INFO] Engine version 0.20.0rc0 found in the config file, assuming engine(s) built by new builder API.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Using C++ session\n",
      "[TensorRT-LLM][INFO] Engine version 0.20.0rc0 found in the config file, assuming engine(s) built by new builder API.\n",
      "[TensorRT-LLM][INFO] MPI size: 1, MPI local size: 1, rank: 0\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [W] Implicitly setting QWenConfig.seq_length = 8192\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [W] Implicitly setting QWenConfig.qwen_type = qwen2\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [W] Implicitly setting QWenConfig.moe_intermediate_size = 0\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [W] Implicitly setting QWenConfig.moe_shared_expert_intermediate_size = 0\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [W] Implicitly setting QWenConfig.tie_word_embeddings = False\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set dtype to float16.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set bert_attention_plugin to auto.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set gpt_attention_plugin to auto.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set gemm_plugin to float16.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set explicitly_disable_gemm_plugin to False.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set gemm_swiglu_plugin to None.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set fp8_rowwise_gemm_plugin to None.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set qserve_gemm_plugin to None.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set identity_plugin to None.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set nccl_plugin to None.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set lora_plugin to None.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set dora_plugin to False.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set weight_only_groupwise_quant_matmul_plugin to None.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set weight_only_quant_matmul_plugin to None.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set smooth_quant_plugins to True.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set smooth_quant_gemm_plugin to None.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set layernorm_quantization_plugin to None.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set rmsnorm_quantization_plugin to None.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set quantize_per_token_plugin to False.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set quantize_tensor_plugin to False.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set moe_plugin to auto.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set mamba_conv1d_plugin to auto.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set low_latency_gemm_plugin to None.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set low_latency_gemm_swiglu_plugin to None.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set gemm_allreduce_plugin to None.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set context_fmha to True.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set bert_context_fmha_fp32_acc to False.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set paged_kv_cache to True.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set remove_input_padding to True.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set norm_quant_fusion to False.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set reduce_fusion to False.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set user_buffer to False.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set tokens_per_block to 32.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set use_paged_context_fmha to True.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set use_fp8_context_fmha to False.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set fuse_fp4_quant to False.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set multiple_profiles to False.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set paged_state to False.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set streamingllm to False.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set manage_weights to False.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set use_fused_mlp to True.\n",
      "[04/24/2025-13:49:51] [TRT-LLM] [I] Set pp_reduce_scatter to False.\n",
      "[TensorRT-LLM][INFO] Engine version 0.20.0rc0 found in the config file, assuming engine(s) built by new builder API.\n",
      "[TensorRT-LLM][INFO] Refreshed the MPI local session\n",
      "[TensorRT-LLM][INFO] MPI size: 1, MPI local size: 1, rank: 0\n",
      "[TensorRT-LLM][INFO] Rank 0 is using GPU 0\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxNumSequences: 1\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxBatchSize: 1\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxBeamWidth: 1\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxSequenceLen: 32768\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxDraftLen: 0\n",
      "[TensorRT-LLM][INFO] TRTGptModel mMaxAttentionWindowSize: (32768) * 28\n",
      "[TensorRT-LLM][INFO] TRTGptModel enableTrtOverlap: 0\n",
      "[TensorRT-LLM][INFO] TRTGptModel normalizeLogProbs: 1\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxNumTokens: 8192\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxInputLen: 8192 = min(maxSequenceLen - 1, maxNumTokens) since context FMHA and usePackedInput are enabled\n",
      "[TensorRT-LLM][INFO] TRTGptModel If model type is encoder, maxInputLen would be reset in trtEncoderModel to maxInputLen: min(maxSequenceLen, maxNumTokens).\n",
      "[TensorRT-LLM][INFO] Capacity Scheduler Policy: GUARANTEED_NO_EVICT\n",
      "[TensorRT-LLM][INFO] Context Chunking Scheduler Policy: None\n",
      "[TensorRT-LLM][INFO] Loaded engine size: 14549 MiB\n",
      "[TensorRT-LLM][INFO] Engine load time 12997 ms\n",
      "[TensorRT-LLM][INFO] Inspecting the engine to identify potential runtime issues...\n",
      "[TensorRT-LLM][INFO] The profiling verbosity of the engine does not allow this analysis to proceed. Re-build the engine with 'detailed' profiling verbosity to get more diagnostics.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 1016.01 MiB for execution context memory.\n",
      "[TensorRT-LLM][INFO] gatherContextLogits: 0\n",
      "[TensorRT-LLM][INFO] gatherGenerationLogits: 0\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 14541 (MiB)\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 1.24 MB GPU memory for runtime buffers.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 2.88 MB GPU memory for decoder.\n",
      "[TensorRT-LLM][INFO] Memory usage when calculating max tokens in paged kv cache: total: 19.60 GiB, available: 4.16 GiB\n",
      "[TensorRT-LLM][INFO] Number of blocks in KV cache primary pool: 2193\n",
      "[TensorRT-LLM][INFO] Number of blocks in KV cache secondary pool: 0, onboard blocks to primary memory before reuse: true\n",
      "[TensorRT-LLM][INFO] Max KV cache pages per sequence: 1024 [window size=32768]\n",
      "[TensorRT-LLM][INFO] Number of tokens per block: 32.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 3.75 GiB for max tokens in paged KV cache (70176).\n",
      "[04/24/2025-13:50:05] [TRT-LLM] [I] Load engine takes: 13.959648132324219 sec\n",
      "Input [Text 0]: \"<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Write a detailed explanation how to solve the world hunger ploblem<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\"\n",
      "Output [Text 0 Beam 0]: \"Solving the world hunger problem is a complex and multifaceted challenge that requires a comprehensive approach involving various stakeholders, including governments, international organizations, NGOs, and local communities. Here’s a detailed explanation of how to address this issue:\n",
      "\n",
      "### 1. **Understanding the Scale and Causes of Hunger**\n",
      "   - **Data Collection and Analysis:** Gather comprehensive data on food production, distribution, and consumption. Identify regions and populations most affected by hunger.\n",
      "   - **Root Causes:** Analyze the underlying causes of hunger, such as poverty, conflict, climate change, and economic instability.\n",
      "\n",
      "### 2. **Improving Food Production and Distribution**\n",
      "   - **Agricultural Development:** Invest in agricultural research and development to improve crop yields and resilience. Promote sustainable farming practices that enhance soil health and water management.\n",
      "   - **Infrastructure:** Develop and maintain infrastructure for transportation and storage to reduce food waste and ensure timely delivery to markets.\n",
      "   - **Market Access:** Provide small-scale farmers with access to markets, credit, and technology to increase their productivity and income.\n",
      "\n",
      "### 3. **Enhancing Food Security Policies**\n",
      "   - **Social Safety Nets:** Implement and expand social safety nets such as food assistance programs, cash transfers, and school feeding programs to support vulnerable populations.\n",
      "   - **Food Subsidies:** Provide subsidies for essential food items to make them more affordable for low-income households.\n",
      "   - **Policy Advocacy:** Advocate for policies that promote food security at national and international levels, such as trade policies that support local farmers and fair trade practices.\n",
      "\n",
      "### 4. **Addressing Climate Change and Environmental Degradation**\n",
      "   - **Climate Resilience:** Develop and implement climate-resilient agricultural practices, such as drought-resistant crops and water-efficient irrigation systems.\n",
      "   - **Conservation:** Promote land and water conservation practices to prevent soil erosion and maintain ecosystem health.\n",
      "   - **Renewable Energy:** Encourage the use of renewable energy sources to reduce the carbon footprint of agriculture and food production.\n",
      "\n",
      "### 5. **Promoting Education and Awareness**\n",
      "   - **Nutrition Education:** Educate communities about proper nutrition and healthy eating habits to improve dietary quality.\n",
      "   - **Agricultural Education:** Provide education and training for farmers on modern agricultural techniques and sustainable practices.\n",
      "   - **Public Awareness:** Raise awareness about the causes and consequences of hunger through media campaigns and community outreach programs.\n",
      "\n",
      "### 6. **Fostering International Cooperation and Aid**\n",
      "   - **Global Partnerships:** Strengthen partnerships between countries, international organizations, and NGOs to share knowledge, resources, and best practices.\n",
      "   - **Aid and Funding:** Secure funding from governments, private sector, and international donors to support hunger alleviation programs.\n",
      "   - **Humanitarian Aid:** Provide emergency food aid and support to regions affected by natural disasters, conflicts, and other crises.\n",
      "\n",
      "### 7. **Innovative Solutions and Technology**\n",
      "   - **Technology Integration:** Utilize technology such as precision agriculture, remote sensing, and digital platforms to improve food production and distribution.\n",
      "   - **Biotechnology:** Develop and deploy biotechnological solutions to enhance crop yields and resistance to pests and diseases.\n",
      "   - **Blockchain:** Use blockchain technology to improve transparency and traceability in the food supply chain, reducing waste and ensuring fair trade practices.\n",
      "\n",
      "### 8. **Empowering Local Communities**\n",
      "   - **Community Engagement:** Involve local communities in decision-making processes related to food security and resource management.\n",
      "   - **Empowerment Programs:** Implement programs that empower women, youth, and marginalized groups to participate in food production and economic activities.\n",
      "   - **Local Food Systems:** Support the development of local food systems that prioritize the production and consumption of locally grown food.\n",
      "\n",
      "### 9. **Monitoring and Evaluation**\n",
      "   - **Data Collection:** Continuously collect and analyze data to monitor progress and identify areas for improvement.\n",
      "   - **Impact Assessment:** Conduct regular impact assessments to evaluate the effectiveness of interventions and make necessary adjustments.\n",
      "   - **Reporting:** Publish reports and share findings with stakeholders to promote transparency and accountability.\n",
      "\n",
      "### 10. **Long-Term Sustainability**\n",
      "   - **Policy Sustainability:** Ensure that policies and programs are sustainable and can be maintained over the long term.\n",
      "   - **Community Sustainability:** Build the capacity of local communities to manage and sustain food security initiatives.\n",
      "   - **Economic Sustainability:** Promote economic growth and development to create a stable foundation for food security.\n",
      "\n",
      "### Conclusion\n",
      "Solving the world hunger problem requires a coordinated and sustained effort from multiple sectors and stakeholders. By addressing the root causes of hunger, improving food production and distribution, enhancing policies, and fostering international cooperation, we can make significant progress in reducing hunger and ensuring food security for all.\"\n",
      "[TensorRT-LLM][INFO] Refreshed the MPI local session\n",
      "CPU times: user 607 ms, sys: 177 ms, total: 784 ms\n",
      "Wall time: 50 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Same\n",
    "!python3 TensorRT-LLM/examples/run.py \\\n",
    "    --engine_dir \"TensorRT-LLM/examples/qwen/7B/trt_engines/fp16/1-gpu/\" \\\n",
    "    --max_output_len 2048 \\\n",
    "    --tokenizer_dir \"TensorRT-LLM/examples/qwen/7B/\" \\\n",
    "    --input_text \"Write a detailed explanation how to solve the world hunger ploblem\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e60ef3-c9f0-4804-b607-6c54c6719f16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorRT-LLM] TensorRT-LLM version: 0.17.0.post1\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "[TensorRT-LLM] TensorRT-LLM version: 0.17.0.post1\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "[TensorRT-LLM][INFO] Engine version 0.17.0.post1 found in the config file, assuming engine(s) built by new builder API.\n",
      "[TensorRT-LLM][INFO] Refreshed the MPI local session\n",
      "[TensorRT-LLM][INFO] MPI size: 1, MPI local size: 1, rank: 0\n",
      "[TensorRT-LLM][INFO] Rank 0 is using GPU 0\n",
      "[TensorRT-LLM][WARNING] Fix optionalParams : KV cache reuse disabled because model was not built with paged context FMHA support\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxNumSequences: 1\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxBatchSize: 1\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxBeamWidth: 1\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxSequenceLen: 4096\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxDraftLen: 0\n",
      "[TensorRT-LLM][INFO] TRTGptModel mMaxAttentionWindowSize: (4096) * 28\n",
      "[TensorRT-LLM][INFO] TRTGptModel enableTrtOverlap: 0\n",
      "[TensorRT-LLM][INFO] TRTGptModel normalizeLogProbs: 0\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxNumTokens: 4096\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxInputLen: 4095 = min(maxSequenceLen - 1, maxNumTokens) since context FMHA and usePackedInput are enabled\n",
      "[TensorRT-LLM][INFO] TRTGptModel If model type is encoder, maxInputLen would be reset in trtEncoderModel to maxInputLen: min(maxSequenceLen, maxNumTokens).\n",
      "[TensorRT-LLM][INFO] Capacity Scheduler Policy: GUARANTEED_NO_EVICT\n",
      "[TensorRT-LLM][INFO] Context Chunking Scheduler Policy: None\n",
      "[TensorRT-LLM][INFO] Loaded engine size: 14548 MiB\n",
      "[TensorRT-LLM][INFO] Inspecting the engine to identify potential runtime issues...\n",
      "[TensorRT-LLM][INFO] The profiling verbosity of the engine does not allow this analysis to proceed. Re-build the engine with 'detailed' profiling verbosity to get more diagnostics.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 556.01 MiB for execution context memory.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 14541 (MiB)\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 706.07 KB GPU memory for runtime buffers.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 2.45 MB GPU memory for decoder.\n",
      "[TensorRT-LLM][INFO] Memory usage when calculating max tokens in paged kv cache: total: 19.60 GiB, available: 4.62 GiB\n",
      "[TensorRT-LLM][INFO] Number of blocks in KV cache primary pool: 1218\n",
      "[TensorRT-LLM][INFO] Number of blocks in KV cache secondary pool: 0, onboard blocks to primary memory before reuse: true\n",
      "[TensorRT-LLM][INFO] KV cache block reuse is disabled\n",
      "[TensorRT-LLM][INFO] Max KV cache pages per sequence: 64\n",
      "[TensorRT-LLM][INFO] Number of tokens per block: 64.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 4.16 GiB for max tokens in paged KV cache (77952).\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m9633\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://localhost:8000\u001b[0m (Press CTRL+C to quit)\n",
      "\u001b[32mINFO\u001b[0m:     ::1:60538 - \"\u001b[1mGET /v1/models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42224 - \"\u001b[1mGET /v1/models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42224 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:40412 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[31m400 Bad Request\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:44274 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!trtllm-serve --tokenizer \"TensorRT-LLM/examples/qwen/7B\" --max_batch_size 1 --max_num_tokens 4096 --max_seq_len 4096 \"TensorRT-LLM/examples/qwen/7B/trt_engines/fp16/1-gpu/\"\n",
    "\n",
    "# !trtllm-serve --max_seq_len 8192 \"Qwen/Qwen2.5-7B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef005d8f-b6d9-404f-ba13-d409a8111177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-24 13:55:00,269 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
      "[TensorRT-LLM] TensorRT-LLM version: 0.20.0rc0\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Usage: trtllm-serve [OPTIONS] COMMAND [ARGS]...\n",
      "\n",
      "Options:\n",
      "  --help  Show this message and exit.\n",
      "\n",
      "Commands:\n",
      "  disaggregated             Running server in disaggregated mode\n",
      "  disaggregated_mpi_worker  Launching disaggregated MPI worker\n",
      "  serve                     Running an OpenAI API compatible server\n"
     ]
    }
   ],
   "source": [
    "# !trtllm-serve \"TensorRT-LLM/examples/qwen/7B/trt_engines/fp16/1-gpu/\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
